---
title: "Taller de Tidymodels"
execute: 
  error: true
format:
  revealjs:
    #incremental: true  
    scrollable: true
    transition: slide
---

## ¬°Hola! {auto-animate="true"}

::: incremental
-   Me llamo Aitor Gonz√°lez.

-   Soy estad√≠stico en la Unidad de Bioestad√≠stica de la Facultad de Medicina de la UAB.
:::

## ¬°Hola! {auto-animate="true"}

-   Me llamo Aitor Gonz√°lez.

-   Soy estad√≠stico en la Unidad de Bioestad√≠stica de la Facultad de Medicina de la UAB.

-   Tambi√©n explico cosas de estad√≠stica en mi canal de YouTube [*ReEstimando*](https://www.youtube.com/@ReEstimando).

![](Imagenes/Logo_reestimando_logo_YT.jpg){width="300" fig-align="center"}

# 2. ¬øPorqu√© Tidymodels?

## 2.1. Un poquito de historia {auto-animate="true"}

![](Imagenes/Max_Kuhn.jpeg){width="400" height="400" margin-top="200px" margin-bottom="0px" padding="0px" fig-align="center"}

<p style="text-align: center; margin-top:0; padding:0">Max Kuhn</p>

## 2.1. Un poquito de historia {auto-animate="true"}

::: columns
::: {.column width="30%"}
![](Imagenes/Max_Kuhn.jpeg){width="300" fig-align="center"}
:::

::: {.column .incremental width="70%"}
-   En 2008 desarroll√≥ la librer√≠a **Caret**, la cual asent√≥ las bases del modelado estad√≠stico en R.
:::
:::

## 2.1. Un poquito de historia {auto-animate="true"}

::: columns
::: {.column width="30%"}
![](Imagenes/Max_Kuhn.jpeg){fig-align="center" width="300"}

![](Imagenes/Paper_Caret.jpeg){width="600" fig-align="center" margin-top="0"}
:::

::: {.column width="70%"}
-   En 2008 desarroll√≥ la librer√≠a **Caret**, la cual asent√≥ las bases del modelado estad√≠stico en R.

-   El paquete cuenta con m√°s de 7.000 citas en publicaciones acad√©micas.
:::
:::

## 2.1. Un poquito de historia {auto-animate="true"}

::: columns
::: {.column width="30%"}
<div style="margin-top:100px">
![](Imagenes/Caret_logo.png){width="300" fig-align="center"}
</div>
:::

::: {.column .incremental width="70%"}
-   Caret fue un √©xito debido a que unific√≥ la sint√°xis de los principales paquetes de modelado en R.

-   Adem√°s, inclu√≠a varias funciones para automatziar procesos necesarios en cualquier modelo.

-   Por ejemplo, una funci√≥n que haga una partici√≥n en los datos: un conjunto de entrenamiento y otro de prueba (training y test).
:::
:::

## 2.1. Un poquito de historia (2) {auto-animate="true"}

::: columns
::: {.column width="20%"}
![](Imagenes/Hadley_Wickham.jpeg){width="300" fig-align="center"}
:::

::: {.column .incremental width="80%"}
-   En 2012 aparece el entorno de Tidyverse de la mano de **Hadley Wickham**.
:::
:::

## 2.1. Un poquito de historia (2) {auto-animate="true"}

::: columns
::: {.column width="20%"}
![](Imagenes/Hadley_Wickham.jpeg){.fade-out width="150" fig-align="center"}

![](Imagenes/Paquetes_Tidyverse.jpeg){.fade-in width="300" fig-align="center"}
:::

::: {.column width="80%"}
-   En 2012 aparece el entorno de Tidyverse de la mano de **Hadley Wickham**.

-   Tidyverse proporciona m√∫ltiples herramientas para poder manipular datos facilmente.

::: incremental
-   Se ha convertido en uno de los paquetes m√°s populares de R y de la ciencia de datos.

-   Tiene una sint√°xis f√°cil y fluida que se enfoca m√°s en la resolver necesidades que en la programaci√≥n.
:::
:::
:::

## 2.1. Un poquito de historia (3) {auto-animate="true"}

::: {layout-ncol="2"}
![](Imagenes/Hadley_Wickham.jpeg){fig-align="center" width="140"}

![](Imagenes/Max_Kuhn.jpeg){fig-align="center" width="210"}
:::

-   En 2019 Max Kuhn se une al *Tidyverso* y comienza a crear Tidymodels junto con el equipo de Hadley Wickham.

## 2.1.Un poquito de historia (3) {auto-animate="true"}

::: {layout-ncol="3"}
![](Imagenes/Hadley_Wickham.jpeg){fig-align="center" width="140"}

![](Imagenes/Max_Kuhn.jpeg){fig-align="center" width="210"}

![](Imagenes/Julia_Silge.jpeg){fig-align="center" width="210"}
:::

-   En 2019 Max Kuhn se une al *Tidyverso* y comienza a crear Tidymodels junto con el equipo de Hadley Wickham.

-   Actualmente **Julia Silge** es, junto con Max Kuhn, la principal desarrolladora de Tidymodels.

## 2.2. Un breve ejemplo {auto-animate="true"}

```{r echo=FALSE}
library(glmnet)
library(tidymodels)
library(h2o)
```

::: columns
::: {.column width="50%"}
-   Con glmnet:

```{r glmnet-2, eval=FALSE, echo=TRUE, results='hide'}
model <-
 glmnet(
   as.matrix(mtcars[2:11]),
   mtcars$mpg
 )
```
:::

::: {.column width="50%"}
-   Con Tidymodels:

```{r tidymod-1, eval=FALSE, echo=TRUE, results='hide'}
model <-
 linear_reg(penalty = 0, mixture = 0) %>%
 set_engine("glmnet") %>%
 fit(mpg ~ ., mtcars)

```
:::
:::

## 2.2. Un breve ejemplo (2) {auto-animate="true"}

::: columns
::: {.column width="50%"}
-   Con H2O:

```{r h2o-1, eval=FALSE, echo=TRUE, results='hide'}
h2o::h2o.init()

as.h2o(mtcars, "mtcars")

model <-
 h2o.glm(
   x = colnames(mtcars[2:11]),
   y = "mpg",
   "mtcars"
 )
```
:::

::: {.column width="50%"}
-   Con Tidymodels:

```{r tidymod-2, eval=FALSE, echo=TRUE, results='hide'}
model <-
 linear_reg() %>%
 set_engine("h2o") %>%
 fit(mpg ~ ., mtcars)
```
:::
:::

## 2.3. ¬øQu√© aporta Tidymodels?

::: incremental
<div style="font-size:32px;">
-   **No m√°s Caret**: el paquete no tendr√° m√°s actualizaciones, s√≥lo mantenimiento

-   **Consistencia**: Misma sint√°xis para todos los procesos

-   **Replicabilidad**

-   **Comunicaci√≥n**: Los outputs de Tidymodels siguen la l√≥gica de Tidyverse

-   **Exportabilidad**: Junto a Plumber y Vetiver se pueden poner los modelos en producci√≥n

-   **Posibilidades**: Tidymodels da soporte a 155 modelos, incluyendo modelos para datos censurados o series temporales
</div>
:::


# 3. ¬øDe qu√© se compone Tidymodels?

## 3. ¬øDe qu√© se compone Tidymodels? {auto-animate="true"}

-   El flujo de trabajo de Tidymodels es el siguiente:

![](Imagenes/Flujo_tidymodels.png){fig-align="center"}

## 3. ¬øDe qu√© se compone Tidymodels? {auto-animate="true"}

-   La idea detr√°s consiste en una sola sintaxis que resuma todo este flujo de trabajo.

::: incremental
<div style="font-size:32px; margin-left:40px;">
1.  Partici√≥n de los datos

2.  Pre-procesamos los datos de entrenamiento

3.  Buscamos los hiper-par√°metros ideales para nuestros datos en el modelo

4.  Ajustamos el modelo

5.  Comprobamos con las m√©tricas

6.  Vemos si funciona con unos datos que el modelo no haya visto
</div>
:::

## 3. ¬øDe qu√© se compone Tidymodels? {auto-animate="true"}

::: columns
::: {.column width="30%"}
<div style="margin-top:100px">
![](Imagenes/paquetes_tidymodels.png){fig-align="center" width="500"}
</div>
:::

::: {.column width="70%"}
::: incremental
<div style="font-size:30px;">
-   **Rsample**: Funciones para crear distintos tipos de remuestreos.
-   **Recipes**: Preprocesador de datos, simplifica el proceso de preparado de datos para cualquier modelo.
-   **Parsnip**: Interfaz para ajustar modelos, hasta 155 tipos a fecha de esta presentaci√≥n
-   **Yarstick**: Creaci√≥n de m√©tricas para la evaluaci√≥n de un modelo.
-   **Workflows**: Cohesi√≥n interna de tidymodels.
-   **Tune & Dials**: Creaci√≥n y gesti√≥n de hiper-par√°metros de cualquier modelo presente en Tidymodels.
</div>
:::
:::
:::


# 4. Presentando los datos

## 4.1. Caso pr√°ctico {auto-animate="true"}

::: incremental
<div style="font-size:30px;">
-   Somos unos expertos en aprendizaje autom√°tico y nos piden ayuda de un hospital üè•

-   Al parecer, nuestra variable, *Var_respuesta*, es una enfermedad complicada de diagnosticar.

-   Los m√©dicos han hecho un estudio caso-control en el que se miran unas cuantas variables; y al cabo de un a√±o se da un diagnostico definitivo de *Var_respuesta*.

-   Haremos una modelizaci√≥n de tipo clasificatoria binaria, es decir de 2 niveles: o tiene la enfermedad o no la tiene. üëç/üëé

-   Adem√°s de de nuestra variable *Var_respuesta*, tenemos otras 25 variables: de cognici√≥n, bioqu√≠micas, gen√©ticas, etc.
</div>
:::

## 4.1. Caso pr√°ctico {auto-animate="true"}

```{r caso-practico-1, echo=TRUE}

# Librer√≠as | Carga de datos 

if(!require('pacman')){install.packages('pacman')}
pacman::p_load(
  readxl,      # Lectura de los datos
  tidyverse ,  # Acceso al entorno de procesamiento "tidyverse"
  tidymodels,  # Acceso al entorno de modelado "tidymodels"
  agua,        # Modelaje de entorno h2o 
  themis,      # Soporte de la librer√≠a "recipes" para a√±adir sobremuestreo
  skimr,       # Descriptiva r√°pida de datos  
  naniar,
  ranger,
  knitr        # Renderizaci√≥n en html para este formato
)
```

## 4.1. Caso pr√°ctico {auto-animate="true"}

-   Antes de empezar, podemos ver la estructura del dataset:

```{r caso-practico-1-2, echo=FALSE}
datos <- read_xlsx('Data/datos.xlsx') %>% 
  filter(!is.na(Var_respuesta) ) %>% 
  mutate(Var_respuesta= as_factor(Var_respuesta))


datos %>% 
  head(.,5) %>% 
  knitr::kable()
```

## 4.1. Caso pr√°ctico {auto-animate="true"}

```{r caso-practico-2, echo=FALSE}
# Valoraci√≥n general
skimr::skim(datos)
```

## 4.1. Caso pr√°ctico {auto-animate="true"}

```{r caso-practico-4, echo=TRUE}
naniar::gg_miss_upset(datos, nsets = 10, nintersects = 50)
```


# 5. Preparando la partici√≥n de datos con Rsample


## 5.1. Cuidado con los datos {auto-animate="true"}

::: incremental
-   Es necesario controlar qu√© datos utilizamos en el entrenamiento del modelo para que cuando se use en producci√≥n, el modelo sea capaz de responder.
:::

## 5.1. Cuidado con los datos {auto-animate="true"}

-   Es necesario controlar qu√© datos utilizamos en el entrenamiento del modelo para que cuando se use en producci√≥n, el modelo sea capaz de responder.

![](Imagenes/meme_spiderman_remuestreo.jpeg){fig-align="center"}

## 5.1.1. Remuestreo con *RSample* {auto-animate="true"}

-   Crearemos una partici√≥n de la muestra en unos datos para ‚Äúentrenar‚Äù el modelo y la otra para ‚Äúevaluarlo‚Äù.

::: {layout-ncol="2"}
![](Imagenes/esquema_training_test.jpeg){fig-align="center" width="300"}

![](Imagenes/logo_rsample.png){fig-align="center" width="198"}
:::

## 5.1.2. Algunas funciones importantes en *RSample* {auto-animate="true"}

-   `initial_split`: Principal funci√≥n dentro de la librer√≠a. Crea un objeto ‚Äúsplit‚Äù sencillo para hacer una partici√≥n de datos.

```{r echo=TRUE}
initial_split(datos, prop = 0.6 )

```

## 5.1.2. Algunas funciones importantes en *RSample* {auto-animate="true"}

-   `bootstraps`: Crea un tibble con n cantidad de objetos ‚Äúsplit‚Äù y su correspondiente identificador. En las muestras tipo bootsrap, los individuos pueden aparecer m√°s de una vez en el conjunto de datos.

```{r echo=TRUE}
rsample::bootstraps(datos,5)
```

## 5.1.2. Algunas funciones importantes en *RSample* {auto-animate="true"}

-   `rolling_origin`: Crea un tibble con n cantidad de objetos ‚Äúsplit‚Äù y su correspondiente identificador. En este caso, los remuestreos no son aleatorios y cada partici√≥n contiene datos consecutivos.

```{r echo=TRUE, eval=FALSE}
rolling_origin(datos, initial = 20, assess = 20, skip = 40, cumulative = FALSE)
```

## 5.1.2. Algunas funciones importantes en *RSample* {auto-animate="true"}

```{r echo=TRUE, eval=FALSE}
rolling_origin(datos, initial = 20, assess = 20, skip = 40, cumulative = FALSE)
rolling_origin(datos, initial = 50, assess = 20, skip = 70, cumulative = FALSE)
rolling_origin(datos, initial = 20, assess = 20, skip = 40, cumulative = TRUE)
rolling_origin(datos, initial = 50, assess = 50, skip = 70, cumulative = TRUE)
```

```{r echo=FALSE}
ggpubr::ggarrange(
  rolling_origin(datos, initial = 20, assess = 20, skip = 40, cumulative = FALSE) %>% 
    tidy() %>% 
    ggplot(aes(x = Resample, y = Row, fill = Data)) +
    geom_tile() + 
    scale_fill_manual( values = c('cyan','orange')) +
    scale_y_continuous(breaks = seq(0, 300, by = 50)) +
    theme_minimal()
  ,
  rolling_origin(datos, initial = 50, assess = 20, skip = 70, cumulative = FALSE) %>% 
    tidy() %>% 
    ggplot(aes(x = Resample, y = Row, fill = Data)) +
    geom_tile()  + 
    scale_fill_manual( values = c('cyan','orange')) +
    scale_y_continuous(breaks = seq(0, 300, by = 50)) +
    theme_minimal()
  ,
  rolling_origin(datos, initial = 20, assess = 20, skip = 40, cumulative = T) %>% 
    tidy() %>% 
    ggplot(aes(x = Resample, y = Row, fill = Data)) +
    geom_tile()  + 
    scale_fill_manual( values = c('cyan','orange')) +
    scale_y_continuous(breaks = seq(0, 300, by = 50)) +
    theme_minimal()
  ,
  rolling_origin(datos, initial = 50, assess = 50, skip = 70, cumulative = T) %>% 
    tidy() %>% 
    ggplot(aes(x = Resample, y = Row, fill = Data)) +
    geom_tile()  + 
    scale_fill_manual( values = c('cyan','orange')) +
    scale_y_continuous(breaks = seq(0, 300, by = 50)) +
    theme_minimal()
  ,
  common.legend = T, legend = 'bottom'
)
```

## 5.2. Estratificaci√≥n {auto-animate="true"}

::: incremental
-   Como las particiones son aleatorias, puede que uno de los conjuntos contenga m√°s datos de un tipo que de otro.

-   Idealmente querr√≠amos que el conjunto de entrenamiento y el de evaluaci√≥n contuvieran la misma proporci√≥n de las categor√≠as.
:::

## 5.2. Estratificaci√≥n {auto-animate="true"}

```{r echo=TRUE}
set.seed(4147)
# Splits ----

## Sin estratificar 
Split_datos_no_strat <- initial_split(datos, prop = 0.70 )
# Put 3/4 of the data into the training set 
# üî¥ Initial_split con datos estratificados üî¥
## Estratificados 
Split_datos_strat <- initial_split(datos, prop = 0.70, strata = Var_respuesta)
```

## 5.2. Estratificaci√≥n {auto-animate="true"}

```{r echo=FALSE}

ggpubr::ggarrange(
  ggpubr::ggarrange(
    training(Split_datos_no_strat) %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      ungroup() %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = scales::percent(prop_var_respuesta)), position = position_stack(vjust = 0.5), size = 4) +
    scale_fill_manual( values=c("azure4", "azure3"))
    ,
    testing(Split_datos_no_strat) %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ungroup() %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = scales::percent(prop_var_respuesta)), position = position_stack(vjust = 0.5), size = 4) +
      scale_fill_manual( values=c("azure4", "azure3")),
    common.legend = T, legend = 'bottom'
  ),
  ggpubr::ggarrange(
    training(Split_datos_strat) %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      ungroup() %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = scales::percent(prop_var_respuesta)), position = position_stack(vjust = 0.5), size = 4)+
      scale_fill_manual( values=c("#E86EB7", "#108064"))
    ,
    testing(Split_datos_strat) %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ungroup() %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = scales::percent(prop_var_respuesta)), position = position_stack(vjust = 0.5), size = 4)+
      scale_fill_manual( values=c("#E86EB7", "#108064")), 
    common.legend = T,
    legend = 'bottom'
  ),
  labels = c('No estratificados', 'Estratificados'), label.x = 0.1
) %>%  
  plot()
```

## 5.3. Deshacer la partici√≥n {auto-animate="true"}

```{r echo=TRUE}
# üî¥ Deshacemos las particiones con las funciones training y testing üî¥

Training_datos <- training(Split_datos_strat)
Testing_datos  <- testing(Split_datos_strat)
```

```{r echo=FALSE}
knitr::kable(head(Training_datos,5))
```

## 5.3. Deshacer la partici√≥n {auto-animate="true"}

```{r echo=TRUE}
# üî¥ Deshacemos las particiones con las funciones training y testing üî¥

Training_datos <- training(Split_datos_strat)
Testing_datos  <- testing(Split_datos_strat)
```

```{r echo=FALSE}
knitr::kable(head(Testing_datos,5))
```

## 5.4. Folds, creando conjuntos de validaci√≥n {auto-animate="true"}

```{r echo=TRUE}
# üî¥ Creamos Folds üî¥

Folds_training <- vfold_cv(Training_datos, v = 5 ,strata = Var_respuesta)
```

```{r echo=FALSE}
Folds_training 
```


# 6. Pre-procesamiento de datos con Parsnip

## 6.1. Pre-procesamiento de datos {auto-animate="true"}

::: incremental
1.  Imputaci√≥n
2.  Normalizaci√≥n
3.  Re-equilibraci√≥n de grupos
4.  Conversi√≥n
:::

## 6.1. Pre-procesamiento de datos {auto-animate="true"}

<p style="margin-top:200px">¬øDebemos ejecutar el pre-procesamiento una y otra vez?</p>

## 6.1. Pre-procesamiento de datos {auto-animate="true"}

::: {layout-ncol="2"}
![](Imagenes/meme_kuzco.jpeg){fig-align="center" width="400"}

![](Imagenes/logo_recipes.png){fig-align="center" width="347"}
:::

## 6.2. ¬øC√≥mo cocinamos una receta? {auto-animate="true"}

::: incremental
Para poder elaborar un flujo completo de pre-procesamiento debemos seguir los siguientes pasos:
<div style="font-size:32px">
-   **Recipe**: la base. Requiere de qu√© f√≥rmula queremos para el modelo y de qu√© conjunto de datos partimos. Los datos aqu√≠ incorporados ser√°n los datos de entrenamiento.
-   **Step**: el proceso que queramos aplicar a nuestros datos, cualquier transofrmaci√≥n de datos va aqu√≠.
-   **Prep**: la receta se prepara, es decir, se aplica al conjunto de entrenamiento y se abstrae para un modelado cualquiera.
-   **Bake**: pone el pre-procesamiento en producci√≥n para un conjunto de datos cualquiera.
</div>
:::

## 6.2.1. Hagamos un ejemplo {auto-animate="true"}

```{r echo=TRUE}
Ejemplo_receta <- Training_datos %>% 
  # üî¥ 1. Recipe (crea la f√≥rmula generalizada del modelo) 
  recipe(  
    as.formula("Var_respuesta ~ Cognicion_01 +Cognicion_02+Escala_01+Escala_02"),
    data = . , 
    strata = Var_respuesta )  %>% 
  # üî¥ 2. step
  # üü° En este caso no aplicamos ninguna transformaci√≥n 
  # Step_log
  # üî¥ 3. Prep (asienta la receta y general√≠zala) 
  prep() %>% 
  # üî¥ 4. Bake (pon la receta en producci√≥n/comprueba qu√© tal ha funcionado)
  bake(., new_data=NULL) 
```

```{r echo=FALSE}
Ejemplo_receta %>% 
  head(.,5) %>% 
  knitr::kable()
```

## 6.2.2. Imputaci√≥n m√∫ltiple {auto-animate="true"}

```{r echo=TRUE}
Imputacion_multiple <- Training_datos %>% 
  # üî¥ 1. Recipe (crea la f√≥rmula generalizada del modelo) 
  recipe(  
    as.formula("Var_respuesta ~ Cognicion_01 +Cognicion_02+Escala_01+Escala_02"),
    data = . , 
    strata = Var_respuesta )  %>% 
  # üî¥ 2. Step (haz la transformaci√≥n que requieras) 
  # üü° Este step ser√° la imputaci√≥n por regresi√≥pn lineal con otras covariables
  step_impute_linear( Cognicion_02, impute_with = imp_vars(Cognicion_01, Escala_02) ) %>%
  # üî¥ 3. Prep (asienta la receta y general√≠zala) 
  prep() %>%
  # üî¥ 4. Bake (pon la receta en producci√≥n/comprueba qu√© tal ha funcionado)
  bake(., new_data=NULL) 
```

```{r echo=FALSE}
Imputacion_multiple%>%   
  head(.,5) %>% 
  knitr::kable()
```

## 6.2.3. PCA {auto-animate="true"}

```{r echo=TRUE}
PCA <- Training_datos %>% 
  # üî¥ 1. Recipe (crea la f√≥rmula generalizada del modelo) 
  recipe(  
    as.formula("Var_respuesta ~ Cognicion_01 + Cognicion_02 + Cognicion_03 + Escala_01 + Escala_02  + Escala_03"),
    data = . , 
    strata = Var_respuesta )  %>% 
  # üî¥ 2. Step (haz la transformaci√≥n que requieras) 
  # üü° Este step ser√° la imputaci√≥n por regresi√≥n lineal con otras covariables
  step_impute_bag(all_numeric()) %>% 
  step_pca( Cognicion_01 , Cognicion_02 , Cognicion_03, num_comp = 2,  id = "pca") %>%
  # üî¥ 3. Prep (asienta la receta y general√≠zala) 
  prep() %>%
  # üî¥ 4. Bake (pon la receta en producci√≥n/comprueba qu√© tal ha funcionado)
  tidy(id = "pca") 
```

```{r echo=FALSE}
PCA %>%
  # displayment
  head(.,5) %>% 
  knitr::kable()
```

## 6.2.4. Remuestrear {auto-animate="true"}

```{r echo=TRUE}
# üî¥ Librer√≠a que permite instalar algoritmos y hacer sobremuestreo en Tidymodels.
pacman::p_load(themis)

Training_datos_adasyn <- Training_datos %>% 
  # 1. Recipe (crea la f√≥rmula generalizada del modelo) 
  recipe(  
    as.formula("Var_respuesta ~ Cognicion_01 +Cognicion_02+Escala_01+Escala_02"),
    data = . , 
    strata = Var_respuesta )  %>% 
  #  2. Step (haz la transoformaci√≥n que requieras) 
  #  step de  imputaci√≥n por √°rboles manteniendo la estructura de todas las covariables
  step_impute_bag(all_numeric()) %>% 
  # üî¥ step para sobremuestrear el nivel menor en la variable respuesta.
  step_adasyn(Var_respuesta, over_ratio = 1, neighbors = 10) %>% 
  #  3. Prep (asienta la receta y general√≠zala) 
  prep() %>%
  #  4. Bake (pon la receta en producci√≥n/comprueba qu√© tal ha funcionado)
  bake(., new_data=NULL)
```

## 6.2.4. Remuestrear {auto-animate="true"}

```{r echo=FALSE}
ggpubr::ggarrange(
  Training_datos %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      ungroup() %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(
        aes(label = scales::percent(prop_var_respuesta)), 
        position = position_stack(vjust = 0.5), size = 4) +
    scale_fill_manual( values=c("azure4", "azure3"))
    ,
    Training_datos_adasyn %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ungroup() %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(
        aes(label = scales::percent(prop_var_respuesta)),
        position = position_stack(vjust = 0.5), size = 4) +
      scale_fill_manual( values=c("azure4", "azure3")),
    common.legend = T, legend = 'bottom'
  ,
  labels = c('Normal', 'Sobre Muestreo\n        Adasyn'), label.x = 0.1
) %>%  
  plot()
```

## 6.3. Caso pr√°ctico: Receta {auto-animate="true"}

<div style="margin-top:200px">
$$ \textbf{VarRespuesta} \sim Cognicion01 + Cognicion02 +\\ 
    Cognicion03 + Cognicion04 + Cognicion05 + \\
    Escala01 + Escala02 + Escala03 $$
</div>
## 6.3. Caso pr√°ctico: Receta {auto-animate="true"}

```{r echo=FALSE}
Receta_modelo_1_formula <- Training_datos %>% 
  dplyr::select(
   dplyr::matches('Cog'), dplyr::matches('escala')) %>% 
  names() %>% 
  paste(., collapse = ' + ') %>% 
  paste('Var_respuesta ~' ,.) %>% 
  as.formula() 
```

```{r echo=TRUE}
# üî¥ 1.) Iniciamos la receta
Receta_modelo_1 <- recipe(  
    formula = Receta_modelo_1_formula,
    data = Training_datos , 
    strata = Var_respuesta)  %>% 
  # üî¥ 2.) Steps
    #  üü° 2.1) Eliminamos las variables que contengan m√°s de un 20% de datos perdidos 
  step_filter_missing(all_predictors(),  threshold = 0.1) %>% 
    #  üü° 2.2) Imputamos las variables num√©ricas con un algoritmo de bagged trees
  step_impute_bag(all_numeric(),-all_outcomes()) %>%
    #  üü° 2.3) Normalizamos los datos (restamos la media)
  step_normalize(all_numeric(),-all_outcomes() ) %>% 
    #  üü° 2.4) Escalamos los datos (reducimos a escala entre 0 y 1)
  step_scale(all_numeric(),-all_outcomes()) %>% 
    #  üü° 2.5) Convertimos a Dummy las variables factor (no har√° nada por que no tenemos variables factor)
  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>% 
    #  üü° 2.6) Sobre muestreamos la variable para equilibrar grupos
  step_adasyn(Var_respuesta, over_ratio = 1, neighbors = 10) 

```

```{r echo=FALSE}
#Receta_modelo_1
```


# 7. ¬øD√≥nde est√° el modelo? Creando modelos con Parsnip

## 7.1. Modelos a la carrera con Parsnip {auto-animate="true"}

![](Imagenes/parsnip_img.jpeg){fig-align="center"}

## 7.1. Modelos a la carrera con Parsnip {auto-animate="true"}

![](Imagenes/search_parsnip_models.png){fig-align="center" width="3500"}
<p style="font-size:20px; text-align: center;">[Search Parsnip Models](https://www.tidymodels.org/find/parsnip/)</p>

## 7.2. Empezamos a modelar {auto-animate="true"}

```{r echo=TRUE, eval=FALSE}
# üî¥ Ver los posibles motores de un tipo de modelo
show_engines('logistic_reg') 

```

## 7.2. Empezamos a modelar {auto-animate="true"}

```{r echo=TRUE, eval=FALSE}
logistic_reg(
  engine = "glmnet",
  #Hiperpar√°metros           
  penalty = NULL, mixture = NULL, mode = 'classification')


logistic_reg(
  engine = "glmnet",
  #Hiperpar√°metros           
  penalty = 0, mixture = 0, mode = 'classification')


logistic_reg(
  engine = "glmnet",
  #Hiperpar√°metros           
  penalty = tune(), mixture = tune(), mode = 'classification')
```

## 7.3. Caso pr√°ctico: modelo de Random Forest {auto-animate="true"}

```{r echo=TRUE}
Model_RandomForest <- 
  # üî¥ Especificamos el modelo que queremos, en este caso un random forest
  rand_forest(
  # üü° Ajustamos los hiperpar√°metros 
    mtry = tune(),  trees = tune(),  min_n = tune()) %>% 
  # üî¥ Ponemos el motor, es decir, la librer√≠a por la queremos que se ejecute el modelo
  set_engine("ranger", importance = "impurity") %>% 
  # üî¥ Ajustamos el modo del modelo, es decir, si queremos regresi√≥n o clasifiaci√≥n 
  set_mode("classification")
```


# 8. Unificando todo: Workflow

## 8. Unificando todo: Workflow

::: columns
::: {.column width="20%"}
<div style="margin-top:100px">
![](Imagenes/workflows_logo.png){fig-align="center" width="300"}
</div>
:::

::: {.column width="80%"}
<div style="font-size:32px;">
Un flujo de trabajo (Workflow) es un objeto que puede agrupar preprocesamiento, modelado y postprocesamiento.

-   La preparaci√≥n de la receta y el ajuste del modelo pueden ejecutarse utilizando una √∫nica llamada a `fit()`.
-   Si tiene configuraci√≥n personalizada de par√°metros de ajuste, √©stos pueden definirse utilizando una interfaz m√°s sencilla cuando se combinan con tune.
-   En el futuro, los flujos de trabajo podr√°n a√±adir operaciones de postprocesamiento, como modificar el corte de probabilidad para modelos de dos clases.
</div>
:::
:::

## 8. Unificando todo: Workflow

```{r echo=TRUE}
WF_Receta_modelo_1_Random_forest <- 
  # üî¥ Activamos el workflow 
  workflow() %>% 
  # üî¥ A√±adimos la receta
  add_recipe(Receta_modelo_1) %>% 
  # üî¥ A√±adimos el modelo
  add_model(Model_RandomForest)
```

```{r echo=FALSE}
WF_Receta_modelo_1_Random_forest
```


# 9. Eligiendo m√©tricas con Yardstick {auto-animate="true"}

## 9. Eligiendo m√©tricas con Yardstick {auto-animate="true"}

::: columns
::: {.column width="20%"}
![](Imagenes/yardstick_logo.png){fig-align="center" width="300"}

:::

::: {.column width="80%"}
-   Yardstick es un paquete para estimar lo bien que funcionan los modelos, dentro del workflow o de forma aislada.
<p style="font-size:25px; margin-top:20px"> [M√°s informaci√≥n](https://yardstick.tidymodels.org/articles/custom-metrics.html) </p>
:::
:::

## 9. Eligiendo m√©tricas con Yardstick {auto-animate="true"}

```{r echo=TRUE}
# üî¥ Seleccionamos las m√©tricas que queremos para nuestro modelo
Modelo_Metricas <- metric_set(accuracy, j_index, precision, sensitivity, specificity, roc_auc, f_meas,recall, mcc)
```

```{r echo=FALSE}
Modelo_Metricas
```


# 10. Eligiendo hiperpar√°metros: Tune y Dials

## 10.1. Eligiendo hiperpar√°metros: Tune y Dials {auto-animate="true"}

::: incremental
-   Los paquetes *Tune* y *Dials* son los que nos permiten hacer esta selecci√≥n y control. Aqu√≠ se crea todo un subflujo de trabajo para el ajuste de hiperpar√°metros.
:::

## 10.1. Eligiendo hiperpar√°metros: Tune y Dials {auto-animate="true"}

-   Los paquetes *Tune* y *Dials* son los que nos permiten hacer esta selecci√≥n y control. Aqu√≠ se crea todo un subflujo de trabajo para el ajuste de hiperpar√°metros.

::: {layout-ncol="2"}
![](Imagenes/logo_tune.png){fig-align="center" width="217"}

![](Imagenes/logo_dials.png){fig-align="center" width="217"}
:::

## 10.1. Eligiendo hiperpar√°metros: Tune y Dials {auto-animate="true"}

::: incremental
1.  Creamos unas r√©plicas del conjunto de entrenamiento, Rsample con el objeto *cv_Folds*

2.  Elegimos un modelo con Parsnip y dejamos sus hiperpar√°metros en abierto con `tune()`.

3.  Creamos una cuadr√≠cula (grid) con diferentes posibles combinaciones de los hiperpar√°metros

4.  Ponemos todo junto con la funci√≥n `tune_grid()`.
:::

## 10.2. Montando un espacio de hiperpar√°metros: familia `grid_` {auto-animate="true"}

::: incremental
-   `grid_regular`: har√° todas las combinaciones entre los rangos de la variable.
-   `grid_random`: probar√° hiperpar√°metros aleatoriamente.
-   `grid_max_entropy`: propondr√° una combinaci√≥n de hiperp√°rametros que garantice que se cubrir√° todo el espectro, con fin de evitar m√≠nimos locales en la conversi√≥n de algoritmos.
-   `grid_latin_hypercube`: propondr√° una matriz.
:::

## 10.2.1. Grid_regular() {auto-animate="true"}

-   `Grid_regular` har√° las combinaciones acorde al rango que le hayamos dado. Por defecto hace la combinaci√≥n entre el m√≠nimo, el medio y el m√°ximo de cada valor.

```{r rid-regular-1, echo=TRUE}

grid_regular(
  mtry(range  = c(2, 10)),
  min_n(range = c(2, 10)),
  trees(range = c(2, 10)))

```

## 10.2.1. Grid_regular() {auto-animate="true"}

-   `Grid_regular` har√° las combinaciones acorde al rango que le hayamos dado. Por defecto hace la combinaci√≥n entre el m√≠nimo, el medio y el m√°ximo de cada valor.

```{r rid-regular-2, echo=TRUE}

grid_regular(
  mtry(range  = c(2, 10)),
  min_n(range = c(2, 10)),
  trees(range = c(2, 10)),
  levels = 4) # Podemos ampliar con el par√°metro levels: Ahora el m√≠nimo ser√° el cuantil 33 y el m√°ximo el cuantil 66.

```

## 10.2.2. Grid_max_entropy() {auto-animate="true"}

```{r rid-max-entr, echo=TRUE}

grid_max_entropy(
  mtry(range  = c(1, 4)),
  min_n(range = c(10, 30)),
  trees(range = c(1, 1000)),
  size = 10
  )
```

## 10.3. Hiperpar√°metros: Dial & Tune + Yardstick {auto-animate="true"}

```{r hiperpar-1, echo=TRUE}
WF_hiper_parametros <- 
  # üî¥ tune_grid()üî¥
  tune_grid(
  # Receta de nuestro modelo, incluye el workflow 
  object = WF_Receta_modelo_1_Random_forest,
  # üü° Ponemos los Folds del conjunto de entrenamiento,  
  resamples = Folds_training,
  # üü° Grid con diferentes hiperpar√°metros 
  grid = grid_max_entropy(
    mtry(range  = c(1, 4)),
    min_n(range = c(10, 30)),
    trees(range = c(1, 1000)),
    size = 10),
  # üü° M√©tricas definidas anteriormente
  metrics = Modelo_Metricas, 
  # üü° Con esta opci√≥n podemos guardar las predicciones
  control = control_grid( save_pred = T)
)
```

## 10.3. Hiperpar√°metros: Dial & Tune + Yardstick {auto-animate="true"}

```{r hiperpar-2, echo=TRUE}
WF_hiperparametros_coleccion <- 
  WF_hiper_parametros %>% 
  collect_metrics() # üî¥ Esta es la funci√≥n que retorna las m√©tricas, importante tenerla en cuenta
```

```{r hiperpar-21, echo=FALSE}
WF_hiperparametros_coleccion %>% 
  head(.,5) %>% 
  knitr::kable()
```

## 10.3. Hiperpar√°metros: Dial & Tune + Yardstick {auto-animate="true"}

```{r hiperpar-3, echo=TRUE}
autoplot(WF_hiper_parametros)
```

## 10.3. Hiperpar√°metros: Dial & Tune + Yardstick {auto-animate="true"}

```{r hiperpar-4, echo=FALSE}
WF_hiper_parametros %>% 
  collect_metrics() %>% 
  pivot_longer(cols = c('mtry','trees', 'min_n'), names_to = 'tipo_parametro', values_to = 'valor_parametro') %>% 
  ggplot(aes(valor_parametro,mean, color=mean)) +
  geom_point() +
  facet_grid(.metric~tipo_parametro, scales = 'free' ) +
  scale_color_viridis_b()
```

## 10.3. Hiperpar√°metros: Dial & Tune + Yardstick {auto-animate="true"}

```{r hiperpar-5, echo=TRUE}
WF_hiperparametros_mejor <- 
  WF_hiper_parametros %>% show_best(metric = "roc_auc",n=1)
```

```{r hiperpar-51, echo=FALSE}
WF_hiperparametros_mejor %>% 
  knitr::kable()
```


# 11. La conversi√≥n de todo: `fit()` y `predict()`

## 11. La conversi√≥n de todo: `fit()` y `predict()` {auto-animate="true"}

```{r modelo-fitted, echo=TRUE}
Modelo_fitted <- WF_Receta_modelo_1_Random_forest %>% 
  # üî¥ Finalizamos el flujo de trabajo con la mejor combinaci√≥n de Hiperpar√°metros encontrada
  finalize_workflow(WF_hiperparametros_mejor) %>%
  # üî¥ Finalmente ajustamos con fit()
  fit(data= Training_datos)
```

```{r modelo-fitted-1, echo=FALSE}
Modelo_fitted
```

## 11. La conversi√≥n de todo: `fit()` y `predict()` {auto-animate="true"}

```{r nuevo-caso-modelo, echo=TRUE}
Nuevo_caso <- tibble(
  'Cognicion_01' = 123,
  'Cognicion_02' = 102,
  'Cognicion_03' = 89,
  'Cognicion_04' = 100,
  'Cognicion_05' = 179,
  'Escala_01'    = 12,
  'Escala_02'    = 22,
  'Escala_03'    = 20
)
```

## 11. La conversi√≥n de todo: `fit()` y `predict()` {auto-animate="true"}

::: incremental
La funci√≥n `predict()` de la librer√≠a stats tiene sinergias con Tidymodels y necesita de estos argumentos:

1.  El modelo ya ajustado (model_fitted)

2.  Un tibble/dataframe con la nueva predicci√≥n (debe contener las mismas variables)

3.  Un tipo de predicci√≥n en los casos de clasificaci√≥n, si queremos las probabilidades de cada categor√≠a o la categor√≠a predicha directamente
:::

## 11. La conversi√≥n de todo: `fit()` y `predict()` {auto-animate="true"}

```{r predict-modelo, echo=TRUE}
tibble(
  predict(Modelo_fitted, Nuevo_caso, type = "prob" ),
  predict(Modelo_fitted, Nuevo_caso, type = "class"))
```

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

::: incremental
-   La funci√≥n `last_fit()` permite tener todo unificado en un solo tibble. Recibe directamente los datos de entrenamiento y los datos de evaluaci√≥n y crea sus m√©tricas y sus predicciones.
:::

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

```{r extract-workflow-1, echo=TRUE}
Modelo_1_final <-  WF_Receta_modelo_1_Random_forest %>% 
  # üî¥ Finalizamos el flujo de trabajo con la mejor combinaci√≥n de hiperpar√°metros encontrada
  finalize_workflow(WF_hiperparametros_mejor) %>%
  # üî¥ Ajustamos con last_fit(), poniendo el split de datos inicial y las m√©tricas para las predicciones
  last_fit(Split_datos_strat, metrics = Modelo_Metricas)
```

```{r extract-workflow-11, echo=FALSE}
Modelo_1_final
```

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

::: incremental
-   Contamos con una serie de funciones para extraer todo lo que necesitemos de ellas:

<div style="font-size:35px;">
1.  `extract_fit_engine()`: permite extraer el modelo crudo como si no hubiera sido ajustado con tidymodels.
2.  `collect_metrics()`: extraer un tibble con las m√©tricas del modelo (puestas con metric set)
3.  `collect_predictions()`: extrae un tibble con las predicciones de los datos de evaluaci√≥n.
4.  `extract_workflow()`: Permite extraer todo el flujo del modelo, incluyendo la receta de modo que podemos hacer predicciones.
</div>
:::

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

```{r extract-workflow-2, echo=TRUE}
WF_Receta_modelo_1_Random_forest %>%
  # üî¥ Finalizamos el flujo de trabajo con la mejor combinaci√≥n de hiperpar√°metros encontrada
  finalize_workflow(WF_hiperparametros_mejor) %>%
  # üî¥ Ajustamos con last_fit(), poniendo el split de datos inicial y las m√©tricas para las predicciones
  last_fit(Split_datos_strat, metrics = Modelo_Metricas) %>%
  # üî¥ dentro del flujo de trabajo est√°n la receta y el modelo ya ajustado
  extract_workflow()
```

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

```{r extract-workflow-3, echo=TRUE}
Modelo_1_final %>%
  # üî¥ Con esta funci√≥n podemos recopilar todas las m√©tricas de un modelo
  collect_metrics() %>% 
  select(-.estimator,-.config) %>%  
  mutate(.estimate= round(.estimate,2))

```

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

```{r curva-roc, echo=FALSE}
options(yardstick.event_first = FALSE)

Modelo_1_final %>%  collect_predictions() %>% 
  # üî¥ Esta funcion permite calcular rapidamente la curva roc de un modelo ya ajustado
  roc_curve(truth = Var_respuesta,  .pred_0 ) %>% 
  ggplot(aes( x = 1- specificity ,y= sensitivity ) ) +
  geom_path() +
  geom_abline(intercept=0, slope=1, linetype=3) +
  labs(title= 'Cruva roc del modelo_1_final')
```


# 12. Threshold performance

## 12. Threshold performance {auto-animate="true"}

::: incremental
-   En tidymodels est√° la librer√≠a `probably` con la funci√≥n `threshold_perf()`, la cual nos permite, dadas las predicciones del modelo, ver c√≥mo se desenvuelve este umbral.
:::

## 12. Threshold performance {auto-animate="true"}

```{r Threshold-1, echo=TRUE}
pacman::p_load(probably)

Modelo_1_final_Threshold_performance <- 
  Modelo_1_final %>% 
  collect_predictions() %>% 
  threshold_perf(
    # Le decimos qu√© variable es la verdadera respuesta
    truth = Var_respuesta,  
    # Le decimos qu√© variable es la predicci√≥n
    .pred_1, 
    # Rango en el que se evaluar√° el umbral en cada punto
    thresholds = seq(0.5,1,0.01) )

```

```{r Threshold-11, echo=FALSE}
Modelo_1_final_Threshold_performance %>% 
  head(.,5) %>% 
  mutate(.estimate= round(.estimate,2)) %>% 
  knitr::kable()
```

## 12. Threshold performance {auto-animate="true"}

```{r Threshold-2, echo=FALSE}
Modelo_1_final_Threshold_performance %>% 
  ggplot(aes(x=.threshold,y=.estimate ,color=.metric)) +
  geom_line(size=1.1)+
  scale_color_manual(values=c('purple','blue','pink'))+
  geom_vline(xintercept = 
    Modelo_1_final_Threshold_performance %>%  
      filter(.metric=='j_index') %>%
      arrange(desc(.estimate)) %>% 
      slice(1) %>% 
      pull(.threshold), linetype = 'dashed', size=1.5)+
  theme_minimal()
```


# 13. `workflow_set()` para unificarlas todas

## 13.1. workflow_set() {auto-animate="true"}

-   Supongamos que queremos evaluar 2 recetas con 1 modelo de RF:

<div style="margin-left:50px; font-size:35px;">
1.  Una con solo las variables 'Escala'
2.  Otra con solo las variables 'Cognici√≥n'
</div>

-   Queremos dejarlo abierto a que puedan haber m√°s.

## 13.1. workflow_set() {auto-animate="true"}

```{r todas-recetas, echo=FALSE}
Receta_modelo_Escalas_formula <- Training_datos %>% 
  dplyr::select(dplyr::matches('Esc')) %>% 
  names() %>% 
  paste(., collapse = ' + ') %>% 
  paste('Var_respuesta ~' ,.) %>% 
  as.formula() 

Receta_modelo_Cognicion_formula <- Training_datos %>% 
  dplyr::select(
   dplyr::matches('Cog')) %>% 
  names() %>% 
  paste(., collapse = ' + ') %>% 
  paste('Var_respuesta ~' ,.) %>% 
  as.formula() 
```

```{r todas-recetas-1, echo=TRUE}
  # üî¥ 1.) Iniciamos la receta
Receta_modelo_Escalas <- recipe(  
    formula = Receta_modelo_Escalas_formula,
    data = Training_datos , 
    strata = Var_respuesta)  %>% 
  # üî¥ 2.) Steps
    #  üü° 2.1) Eliminamos las variables que contengan m√°s de un 25% de datos perdidos 
  step_filter_missing(all_predictors(),  threshold = 0.25) %>% 
    #  üü° 2.2) Imputamos las variables num√©ricas con un algoritmo de bagged trees
  step_impute_bag(all_numeric(),-all_outcomes()) %>%
    # 2.3) Normalizamos los datos (restamos la media)
  step_normalize(all_numeric(),-all_outcomes() ) %>% 
    # 2.4) Escalamos los datos (reducimos a escala entre 0 y 1)
  step_scale(all_numeric(),-all_outcomes()) %>% 
    # 2.5) Convertimos a Dummy las variables factor (no har√° nada por que no tenemos variables factor)
  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>% 
    # 2.6) Sobremuestreamos la variable para equilibrar grupos
  step_adasyn(Var_respuesta, over_ratio = 1, neighbors = 10) 


  # üî¥ 1.) Iniciamos la receta
Receta_modelo_Cognicion <- recipe(  
    formula = Receta_modelo_Cognicion_formula,
    data = Training_datos , 
    strata = Var_respuesta)  %>% 
  # üî¥ 2.) Steps
    #  üü° 2.1) Eliminamos las variables que contengan m√°s de un 25% de datos perdidos 
  step_filter_missing(all_predictors(),  threshold = 0.25) %>%
    #  üü° 2.2) Imputamos las variables num√©ricas con un algoritmo de bagged trees
  step_impute_mean(all_numeric(),-all_outcomes()) %>%
    # 2.3) Normalizamos los datos (restamos la media)
  step_normalize(all_numeric(),-all_outcomes() ) %>% 
    # 2.4) Escalamos los datos (reducimos a escala entre 0 y 1)
  step_scale(all_numeric(),-all_outcomes()) %>% 
    # 2.5) Convertimos a Dummy las variables factor (no har√° nada por que no tenemos variables factor)
  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>% 
    # 2.6) Sobremuestreamos la variable para equilibrar grupos
  step_adasyn(Var_respuesta, over_ratio = 1, neighbors = 10)

```

## 13.2. workflow_set() {auto-animate="true"}

```{r workflow-set-1, echo=TRUE}
Receta_list <- list(
  'Modelo_Escalas'    = Receta_modelo_Escalas,
  'Modelo_Cognicion'  = Receta_modelo_Cognicion
  )
  
Model_list <- list(
  RandomForest     = Model_RandomForest
  # ,XGBoost          = Model_XGBoost
  # ,Bagged_Trees     = Model_Bagged_Tree
  )
```

## 13.2. workflow_set() {auto-animate="true"}

```{r workflow-set-2, echo=TRUE}
Wokflows_set <- workflow_set(
  preproc = Receta_list, 
  models = Model_list, 
  # üî¥ La opci√≥n Cross es para que haga todos los cruces de modelos con recetas
  cross = T)
```

```{r workflow-set-21, echo=FALSE}
Wokflows_set
```

## 13.2. workflow_set() {auto-animate="true"}

```{r workflow-set-3, echo=TRUE}
Wokflows_set_map <- 
  Wokflows_set %>% 
  # üî¥ workflow_map() es una funci√≥n que permite ajustar m√∫ltiples flujos de trabajo
    workflow_map(
    resamples = Folds_training, 
    fn = "tune_grid",
    grid = grid_max_entropy(
      mtry(range  = c(1, 4)),
      min_n(range = c(10, 30)),
      trees(range = c(1, 1000)),
      size = 10),
    # verbose = TRUE, 
    metrics = Modelo_Metricas, 
    control = control_grid( save_pred = T),
    # üî¥ para garantizar replicabildiad, podemos fijar la semilla en el proceso
    seed = 2465)
```

```{r workflow-set-31, echo=FALSE}
Wokflows_set_map
```

## 13.2. workflow_set() {auto-animate="true"}

```{r workflow-set-4, echo=T}

# Modelo Escalas ----
Modelo_Escalas_RandomForest_hyperparametros <- Wokflows_set_map %>% 
  # üî¥ Extraemos el flujo correspondiente al modelo que queremos 
  extract_workflow_set_result('Modelo_Escalas_RandomForest') %>%
  # üî¥ podemos ver una tabla con los mejores resultados, acorde a una m√©trica
  show_best(metric = 'sensitivity', n=50) %>% 
  filter(row_number()==1) %>% 
  select(mtry,trees,min_n,.config )

Modelo_Escalas_RandomForest_final <- Wokflows_set_map %>% 
  # üî¥ Extraemos el flujo correspondiente al modelo que queremos 
  extract_workflow('Modelo_Escalas_RandomForest') %>% 
  finalize_workflow(Modelo_Escalas_RandomForest_hyperparametros) %>%
  last_fit(Split_datos_strat, metrics = Modelo_Metricas )

# Modelo Cognici√≥n ----

Modelo_Cognicion_RandomForest_hyperparametros <- Wokflows_set_map %>% 
  extract_workflow_set_result('Modelo_Cognicion_RandomForest') %>%
  show_best(metric = 'sensitivity', n=50) %>% 
  filter(row_number()==1) %>% 
  select(mtry,trees,min_n,.config )

Modelo_Cognicion_RandomForest_final <- Wokflows_set_map %>% 
  extract_workflow('Modelo_Cognicion_RandomForest') %>% 
  finalize_workflow(Modelo_Cognicion_RandomForest_hyperparametros) %>%
  last_fit(Split_datos_strat, metrics = Modelo_Metricas )
```

## 13.3. M√©tricas de todos los modelos {auto-animate="true"}

```{r metricas-1, echo=F}
map2(
  list(
    Modelo_Escalas_RandomForest_final,
    Modelo_Cognicion_RandomForest_final
    ),
  list(
    'Modelo_Escala',
    'Modelo_Cognicion'
    ),
  ~ ..1 %>% 
    collect_metrics() %>% 
    mutate('Modelo'= ..2) %>% 
    select(.metric,.estimate,Modelo )) %>% 
  bind_rows() %>% 
  pivot_wider(names_from = Modelo, values_from = .estimate) %>% 
  knitr::kable()
```

## 13.4. Curvas roc de todos los modelos {auto-animate="true"}

```{r echo=FALSE}
ROC_CURVE_training_todos_los_posibles_modelos <- Wokflows_set_map %>%  
  collect_predictions() %>%
  group_split(wflow_id) %>% 
  set_names(list('Escala','Cognicion')) %>% 
  map(~ .x %>% roc_curve(truth=Var_respuesta , .pred_0)) %>% 
  map2(., list('Escala','Cognicion'),
               ~ .x %>%  mutate(model= .y)) %>% 
         bind_rows() 

Plot_ROC_CURVE_training_todos_los_posibles_modelos <- ROC_CURVE_training_todos_los_posibles_modelos %>% 
  ggplot(aes( x = 1- specificity ,y= sensitivity, color= model ) ) +
  geom_path() +
  geom_abline(intercept=0, slope=1, linetype=3) +
  theme_minimal()
  


ROC_CURVE_training_modelos_finales <- list(
  list('Esc' ,Modelo_Escalas_RandomForest_hyperparametros$.config),
  list('Cog' ,Modelo_Cognicion_RandomForest_hyperparametros$.config)) %>% 
  map(~ Wokflows_set_map %>% 
        collect_predictions() %>% 
        filter(
          str_detect(wflow_id, .x[[1]]) & 
            .config== .x[[2]]
        )) %>% 
  
  map(~ .x %>%
        mutate(Model= str_extract(wflow_id, '^.{2}') ) %>% 
        select(Model,Var_respuesta, .pred_0, .pred_1, .pred_class)
  ) %>% 
  set_names(list('Escala','Cognicion') ) %>%
  map(~ .x %>% roc_curve(truth = Var_respuesta, .pred_0)) %>% 
  map2(., list('Escala','Cognicion'),
       ~ .x %>% mutate(model= .y)) %>% 
  bind_rows() 
  
Plot_ROC_CURVE_training_modelos_finales <- ROC_CURVE_training_modelos_finales %>% 
  ggplot(aes( x = 1- specificity ,y= sensitivity, color= model ) ) +
  geom_path() +
  geom_abline(intercept=0, slope=1, linetype=3) +
  theme_minimal()


ROC_CURVE_testing_modelos_finales <-list(
  Modelo_Escalas_RandomForest_final,
  Modelo_Cognicion_RandomForest_final
  ) %>% 
  map(~.x %>% 
        collect_predictions() %>% 
        roc_curve(Var_respuesta, .pred_0)) %>% 
  map2(., list('Escala','Cognicion'),
       ~ .x %>%  mutate(model= .y)) %>% 
  bind_rows()

Plot_ROC_CURVE_testing_modelos_finales <- ROC_CURVE_testing_modelos_finales %>% 
  ggplot(aes(x = 1- specificity ,y = sensitivity, color = model ) ) +
  geom_path()+
  geom_abline(intercept=0, slope=1, linetype=3) +
  theme_minimal()
```

```{r curvas-roc-todos-modelos, echo=FALSE}
ggpubr::ggarrange( Plot_ROC_CURVE_training_todos_los_posibles_modelos,
                   Plot_ROC_CURVE_training_modelos_finales, 
                   Plot_ROC_CURVE_testing_modelos_finales ,
                   nrow=1 , 
                   common.legend = T,
                   legend="bottom",
                   labels = c('Todos_training','Training','Test'))
```

# 14. SHAP values

## 14. SHAP values

::: incremental
-   Forma de ver la contribuci√≥n de las variables utilizadas en las predicciones.

-   Se pueden utilizar tanto como para el modelo general como en individuos.

-   Algunos paquetes que se encargan de generar shap values son h2o y DALEXTRA.
:::

## 14. SHAP values

```{r shap-values-1, echo=FALSE, output=FALSE}
h2o_start()

modelo_h20 <- rand_forest() %>% 
  set_engine("h2o", max_runtime_secs = 20) %>% 
  set_mode('classification') %>% 
  fit(Var_respuesta ~ ., data = Receta_modelo_Escalas %>% prep() %>% bake(new_data = NULL) )

entrada_h20 <- h2o::as.h2o(
  Receta_modelo_Escalas %>% prep() %>% bake(new_data = NULL) %>%
    dplyr::rename(.outcome = Var_respuesta))
```

```{r shap-values-12, echo=FALSE}
modelo_h20 %>%  
  extract_fit_engine() %>% 
  h2o::h2o.shap_summary_plot(entrada_h20)

```

```{r shap-values-2, echo=FALSE}
h2o_end()
```

# Gracias por vuestra atenci√≥n