---
title: "Taller de Tidymodels"
format:
  revealjs:
    #incremental: true  
    scrollable: true
    transition: slide
    include-in-header: 
      text: |
        <style>
        .titulo-seccion {
          font-size: 50px;
          display: flex;
          justify-content: center;
          align-items: center;
          margin: 0;
          position: absolute;
          top: 45%;
          left: 15%;
        }
        </style>
---

## ¬°Hola! {auto-animate="true"}

::: incremental
-   Me llamo Aitor Gonz√°lez.

-   Soy estad√≠stico en la Unidad de Bioestad√≠stica de la Facultad de Medicina de la UAB.

-   Tambi√©n explico cosas de estad√≠stica en mi canal de YouTube *ReEstimando*.
:::

## ¬°Hola! {auto-animate="true"}

-   Me llamo Aitor Gonz√°lez.

-   Soy estad√≠stico en la Unidad de Bioestad√≠stica de la Facultad de Medicina de la UAB.

-   Tambi√©n explico cosas de estad√≠stica en mi canal de YouTube *ReEstimando*.

![](Imagenes/Logo_reestimando_logo_YT.jpg){width="300" fig-align="center"}

::: titulo-seccion
## 2. ¬øPorqu√© Tidymodels?
:::

## 2.1. Un poquito de historia {auto-animate="true"}

![](Imagenes/Max_Kuhn.jpeg){width="400" height="400" top-margin="200px" fig-align="center"}

## 2.1. Un poquito de historia {auto-animate="true"}

::: columns
::: {.column width="30%"}
![](Imagenes/Max_Kuhn.jpeg){width="300" fig-align="center"}
:::

::: {.column .incremental width="70%"}
-   En 2008 desarroll√≥ la librer√≠a Caret, la cual asent√≥ las bases del modelado estad√≠stico en R.

:::
:::

## 2.1. Un poquito de historia {auto-animate="true"}

::: columns
::: {.column width="30%"}
![](Imagenes/Max_Kuhn.jpeg){width="300" fig-align="center"}

![](Imagenes/Paper_Caret.jpeg){width="600" fig-align="center" margin-top="0"}
:::

::: {.column width="70%"}
-   En 2008 desarroll√≥ la librer√≠a Caret, la cual asent√≥ las bases del modelado estad√≠stico en R.

-   El paquete cuenta con m√°s de 7.000 citas en publicaciones acad√©micas.
:::
:::

## 2.1. Un poquito de historia {auto-animate="true"}

::: columns
::: {.column width="30%"}
![](Imagenes/Caret_logo.png){width="300" fig-align="center"}
:::

::: {.column .incremental width="70%"}
-   Caret fue un √©xito debido a que unific√≥ la sint√°xis de los principales paquetes de modelado en R.

-   Adem√°s, inclu√≠a varias funciones para automatziar procesos necesarios en cualquier modelo.

-   Por ejemplo, una funci√≥n que haga una partici√≥n en los datos: un conjunto de entrenamiento y otro de prueba (training y test).
:::
:::

## 2.1. Un poquito de historia (2) {auto-animate="true"}

::: columns
::: {.column width="30%"}
![](Imagenes/Hadley_Wickham.jpeg){width="300" fig-align="center"}
:::

::: {.column .incremental width="70%"}
-   En 2012 aparece el entorno de Tidyverse de la mano de Hadley Wickham.
:::
:::

## 2.1. Un poquito de historia (2) {auto-animate="true"}

::: columns
::: {.column width="30%"}
![](Imagenes/Hadley_Wickham.jpeg){.fade-out width="150" fig-align="center"}

![](Imagenes/Paquetes_Tidyverse.jpeg){.fade-in width="300" fig-align="center"}
:::

::: {.column width="70%"}
-   En 2012 aparece el entorno de Tidyverse de la mano de Hadley Wickham.

-   Tidyverse proporciona m√∫ltiples herramientas para poder manipular datos facilmente.

::: incremental

-   Se ha convertido en uno de los paquetes m√°s populares de R y de la ciencia de datos.

-   Tiene una sint√°xis f√°cil y fluida que se enfoca m√°s en la resolver necesidades que en la programaci√≥n.
:::
:::
:::

## 2.1. Un poquito de historia (3) {auto-animate="true"}

::: {layout-ncol="2"}
![](Imagenes/Hadley_Wickham.jpeg){width="150" fig-align="center"}

![](Imagenes/Max_Kuhn.jpeg){width="150" fig-align="center"}
:::

-   En 2019 Max Kuhn se une al *Tidyverso* y comienza a crear Tidymodels junto con el equipo de Hadley Wickham.

## 2.1.Un poquito de historia (3) {auto-animate="true"}

::: {layout-ncol="3"}
![](Imagenes/Hadley_Wickham.jpeg){width="150" fig-align="center"}

![](Imagenes/Max_Kuhn.jpeg){width="150" fig-align="center"}

![](Imagenes/Julia_Silge.jpeg){width="150" fig-align="center"}
:::

-   En 2019 Max Kuhn se une al *Tidyverso* y comienza a crear Tidymodels junto con el equipo de Hadley Wickham.

-   Actualmente Julia Silge es, junto con Max Kuhn, la principal desarrolladora de Tidymodels.

## 2.2. Un breve ejemplo {auto-animate="true"}

```{r echo=FALSE}
library(glmnet)
library(tidymodels)
library(h2o)
```

::: columns
::: {.column width="50%"}
-   Con glmnet:

```{r glmnet-2, eval=FALSE, echo=TRUE, results='hide'}
model <-
 glmnet(
   as.matrix(mtcars[2:11]),
   mtcars$mpg
 )
```
:::

::: {.column width="50%"}
-   Con Tidymodels:

```{r tidymod-1, eval=FALSE, echo=TRUE, results='hide'}
model <-
 linear_reg(penalty = 0, mixture = 0) %>%
 set_engine("glmnet") %>%
 fit(mpg ~ ., mtcars)

```
:::
:::

## 2.2. Un breve ejemplo (2) {auto-animate="true"}

::: columns
::: {.column width="50%"}
-   Con H2O:

```{r h2o-1, eval=FALSE, echo=TRUE, results='hide'}
h2o::h2o.init()

as.h2o(mtcars, "mtcars")

model <-
 h2o.glm(
   x = colnames(mtcars[2:11]),
   y = "mpg",
   "mtcars"
 )
```
:::

::: {.column width="50%"}
-   Con Tidymodels:

```{r tidymod-2, eval=FALSE, echo=TRUE, results='hide'}
model <-
 linear_reg() %>%
 set_engine("h2o") %>%
 fit(mpg ~ ., mtcars)
```
:::
:::

## 2.3. ¬øQu√© aporta Tidymodels?

::: incremental
-   **No m√°s Caret**: el paquete no tendr√° m√°s actualizaciones, s√≥lo mantenimiento

-   **Consistencia**: Misma sint√°xis para todos los procesos

-   **Replicabilidad**

-   **Comunicaci√≥n**: Los outputs de Tidymodels siguen la l√≥gica de Tidyverse

-   **Exportabilidad**: Junto a Plumber y Vetiver se pueden poner los modelos en producci√≥n

-   **Posibilidades**: Tidymodels da soporte a 155 modelos, incluyendo modelos para datos censurados o series temporales
:::

::: titulo-seccion
## 3. Presentando los datos
:::

## 3.1. Caso pr√°ctico {auto-animate="true"}

::: incremental
-   Somos unos expertos en aprendizaje autom√°tico y nos piden ayuda de un hospital üè•

-   Al parecer, nuestra variable, *Var_respuesta*, es una enfermedad complicada de diagnosticar.

-   Los m√©dicos han hecho un estudio caso-control en el que se miran unas cuantas variables; y al cabo de un a√±o se da un diagnostico definitivo de *Var_respuesta*.

-   Haremos una modelizaci√≥n de tipo clasificatoria binaria, es decir de 2 niveles: o tiene la enfermedad o no la tiene. üëç/üëé

-   Adem√°s de de nuestra variable *Var_respuesta*, tenemos otras 25 variables: de cognici√≥n, bioqu√≠micas, gen√©ticas, etc.
:::

## 3.1. Caso pr√°ctico {auto-animate="true"}

```{r caso-practico-1, echo=TRUE}

# Librer√≠as | Carga de datos 

if(!require('pacman')){install.packages('pacman')}
pacman::p_load(
  readxl,      # Lectura de los datos
  tidyverse ,  # Acceso al entorno de procesamiento "tidyverse"
  tidymodels,  # Acceso al entorno de modelado "tidymodels"
  agua,        # Modelaje de entorno h2o 
  themis,      # Soporte de la librer√≠a "recipes" para a√±adir sobremuestreo
  skimr,       # Descriptiva r√°pida de datos  
  naniar,
  knitr        # Renderizaci√≥n en html para este formato
)
```

## 3.1. Caso pr√°ctico {auto-animate="true"}

-   Antes de empezar, podemos ver la estructura del dataset:

```{r caso-practico-1-2, echo=FALSE}
datos <- read_xlsx('Data/datos.xlsx') %>% 
  filter(!is.na(Var_respuesta) ) %>% 
  mutate(Var_respuesta= as_factor(Var_respuesta))


datos %>% 
  head(.,10) %>% 
  knitr::kable()
```

## 3.1. Caso pr√°ctico {auto-animate="true"}

```{r caso-practico-2, echo=TRUE}
# Valoraci√≥n general
skimr::skim(datos)
```

## 3.1. Caso pr√°ctico {auto-animate="true"}

```{r caso-practico-3, echo=TRUE}
# Valores perdidos
naniar::vis_miss(datos)
```

## 3.1. Caso pr√°ctico {auto-animate="true"}

```{r caso-practico-4, echo=TRUE}
naniar::gg_miss_upset(datos, nsets = 10, nintersects = 50)
```

::: titulo-seccion
## 4. ¬øDe qu√© se compone Tidymodels?
:::

## 4. ¬øDe qu√© se compone Tidymodels? {auto-animate="true"}

-   El flujo de trabajo de Tidymodels es el siguiente:

![](Imagenes/Flujo_tidymodels.png){fig-align="center"}


## 4. ¬øDe qu√© se compone Tidymodels? {auto-animate="true"}

-   La idea detr√°s consiste en una sola sintaxis que resuma todo este flujo de trabajo.

::: incremental
1.  Partici√≥n de los datos

2.  Pre-procesamos los datos de entrenamiento

3.  Buscamos los hiper-par√°metros ideales para nuestros datos en el modelo

4.  Ajustamos el modelo

5.  Comprobamos con las m√©tricas

6.  Vemos si funciona con unos datos que el modelo no haya visto
:::

## 4. ¬øDe qu√© se compone Tidymodels? {auto-animate="true"}

::: incremental
![](Imagenes/paquetes_tidymodels.png){fig-align="center" width="500"}



- **Rsample**: Funciones para crear distintos tipos de remuestreos.
- **Recipes**: Preprocesador de datos, simplifica el proceso de preparado de datos para cualquier modelo.
- **Parsnip**: Interfaz para ajustar modelos, hasta 155 tipos a fecha de esta presentaci√≥n
- **Yarstick**: Creaci√≥n de m√©tricas para la evaluaci√≥n de un modelo.
- **Workflows**: Cohesi√≥n interna de tidymodels.
- **Tune & Dials**: Creaci√≥n y gesti√≥n de hiper-par√°metros de cualquier modelo presente en Tidymodels.
::: 

::: titulo-seccion
## 5. Preparando la partici√≥n de datos con Rsample
:::  

## 5.1. ¬°Necesitamos datos! {auto-animate="true"}

::: incremental
- Suelen ser dif√≠ciles de conseguir
- Requieren tiempo, esfuerzo y **dinero**
:::

## 5.1. ¬°Necesitamos datos! {auto-animate="true"}

- Suelen ser dif√≠ciles de conseguir
- Requieren tiempo, esfuerzo y **dinero**

:::incremental
- Optimizar el manejo de los datos para abaratar los costes de recopilaci√≥n y mejorar los resultados.
- Tener en cuenta el prop√≥sito del modelo para predecir qu√© pasar√° con datos que todav√≠a no conocemos.
:::

## 5.1. ¬°Necesitamos datos! {auto-animate="true"}

- Optimizar el manejo de los datos para abaratar los costes de recopilaci√≥n y mejorar los resultados.
- Tener en cuenta el prop√≥sito del modelo para predecir qu√© pasar√° con datos que todav√≠a no conocemos.

![](Imagenes/meme_spiderman_remuestreo.jpeg){fig-align="center"}


## 5.1.1. Remuestreo con *RSample* {auto-animate="true"}


- Crearemos una partici√≥n de la muestra en unos datos para ‚Äúentrenar‚Äù el modelo y la otra para ‚Äúevaluarlo‚Äù.

::: {layout-ncol="2"}
![](Imagenes/esquema_training_test.jpeg){fig-align="center"}

![](Imagenes/logo_rsample.png){fig-align="center"}
:::


## 5.1.2. Algunas funciones importantes en *RSample* {auto-animate="true"}

- initial_split: Principal funci√≥n dentro de la librer√≠a. Crea un objeto ‚Äúsplit‚Äù sencillo para hacer una partici√≥n de datos.


```{r echo=TRUE}
initial_split(datos, prop = 0.6 )

```

## 5.1.2. Algunas funciones importantes en *RSample* {auto-animate="true"}

- bootstraps: Crea un tibble con n cantidad de objetos ‚Äúsplit‚Äù y su correspondiente identificador. En las muestras tipo bootsrap, los individuos pueden aparecer m√°s de una vez en el conjunto de datos.


```{r echo=TRUE}
rsample::bootstraps(datos,5)
```

## 5.1.2. Algunas funciones importantes en *RSample* {auto-animate="true"}

- rolling_origin: Crea un tibble con n cantidad de objetos ‚Äúsplit‚Äù y su correspondiente identificador. En este caso, los remuestreos no son aleatorios y cada partici√≥n contiene datos consecutivos.


```{r echo=TRUE}
rolling_origin(datos, initial = 20, assess = 20, skip = 40, cumulative = FALSE)
```


## 5.2. Estratificaci√≥n {auto-animate="true"}

- Como las particiones son aleatorias, puede que uno de los conjuntos contenga m√°s datos de un tipo que de otro.

- Idealmente querr√≠amos que el conjunto de entrenamiento y el de evaluaci√≥n contuvieran la misma proporci√≥n de las categor√≠as.

## 5.2. Estratificaci√≥n {auto-animate="true"}

```{r echo=TRUE}
set.seed(4147)
# Splits ----

## Sin estratificar 
Split_datos_no_strat <- initial_split(datos, prop = 0.70 )
# Put 3/4 of the data into the training set 
# üî¥ Initial_split con datos estratificados üî¥
## Estratificados 
Split_datos_strat <- initial_split(datos, prop = 0.70, strata = Var_respuesta)
```


## 5.2. Estratificaci√≥n {auto-animate="true"}

```{r echo=FALSE}

ggpubr::ggarrange(
  ggpubr::ggarrange(
    training(Split_datos_no_strat) %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      ungroup() %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = scales::percent(prop_var_respuesta)), position = position_stack(vjust = 0.5), size = 4) +
    scale_fill_manual( values=c("azure4", "azure3"))
    ,
    testing(Split_datos_no_strat) %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ungroup() %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = scales::percent(prop_var_respuesta)), position = position_stack(vjust = 0.5), size = 4) +
      scale_fill_manual( values=c("azure4", "azure3")),
    common.legend = T, legend = 'bottom'
  ),
  ggpubr::ggarrange(
    training(Split_datos_strat) %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      ungroup() %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = scales::percent(prop_var_respuesta)), position = position_stack(vjust = 0.5), size = 4)+
      scale_fill_manual( values=c("#E86EB7", "#108064"))
    ,
    testing(Split_datos_strat) %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ungroup() %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(aes(label = scales::percent(prop_var_respuesta)), position = position_stack(vjust = 0.5), size = 4)+
      scale_fill_manual( values=c("#E86EB7", "#108064")), 
    common.legend = T,
    legend = 'bottom'
  ),
  labels = c('No estratificados', 'Estratificados'), label.x = 0.1
) %>%  
  plot()
```


## 5.3. Deshacer la partici√≥n {auto-animate="true"}

```{r echo=TRUE}
# üî¥ Deshacemos las particiones con las funciones training y testing üî¥

Training_datos <- training(Split_datos_strat)
Testing_datos  <- testing(Split_datos_strat)

knitr::kable(head(Training_datos,10))
```


## 5.3. Deshacer la partici√≥n {auto-animate="true"}

```{r echo=TRUE}
# üî¥ Deshacemos las particiones con las funciones training y testing üî¥

Training_datos <- training(Split_datos_strat)
Testing_datos  <- testing(Split_datos_strat)

knitr::kable(head(Testing_datos,10))
```

## 5.4. Folds, creando conjuntos de validaci√≥n {auto-animate="true"}

```{r echo=TRUE}
# üî¥ Creamos Folds üî¥

Folds_training <- vfold_cv(Training_datos, v = 5 ,strata = Var_respuesta)
```


::: titulo-seccion
## 6. Pre-procesamiento de datos con Parsnip
:::  

## 6.1. ¬øQu√© es el pre-procesamiento de datos? {auto-animate="true"}

:::incremental
- El pre-procesamiento es clave en el proceso de creaci√≥n de modelos de machine learning.

- Cuando queremos hacer un data frame para modelizar en aprendizaje autom√°tico hay que tener en cuenta:

1. Hay modelos que no aceptan valores perdidos. 
2. Dependiendo de c√≥mo queramos escalar el modelo los datos deber√°n estar normalizados o no.
3. Re-equilibrar grupos disparejos aumenta la eficiencia de preddici√≥n, aunque no siempre.
4. Reconvertir/transformar variables para maximizar el rendimiento del modelo.
:::

## 6.1. ¬øQu√© es el pre-procesamiento de datos? {auto-animate="true"}

¬øC√≥mo hacemos cuando queramos poner un modelo a grane scala y entren datos a cada momento? 
¬øDebemos ejecutar el pre-procesamiento una y otra vez?


## 6.1. ¬øQu√© es el pre-procesamiento de datos? {auto-animate="true"}

::: {layout-ncol="2"}
![](Imagenes/meme_kuzco.jpeg){fig-align="center"}

![](Imagenes/logo_recipes.png){fig-align="center"}
:::


## 6.2. ¬øC√≥mo cocinamos una receta? {auto-animate="true"}
:::incremental
Para poder elaborar un flujo completo de pre-procesamiento debemos seguir los siguientes pasos:

- **Recipe**: la base. Requiere de qu√© f√≥rmula queremos para el modelo y de qu√© conjunto de datos partimos. Los datos aqu√≠ incorporados ser√°n los datos de entrenamiento.
- **Step**: el proceso que queramos aplicar a nuestros datos, cualquier transofrmaci√≥n de datos va aqu√≠.
- **Prep**: la receta se prepara, es decir, se aplica al conjunto de entrenamiento y se abstrae para un modelado cualquiera.
- **Bake**: pone el pre-procesamiento en producci√≥n para un conjunto de datos cualquiera.
:::


## 6.2.1. Hagamos un ejemplo {auto-animate="true"}

```{r}
Training_datos %>% 
  # üî¥ 1. Recipe (crea la f√≥rmula generalizada del modelo) 
  recipe(  
    as.formula("Var_respuesta ~ Cognicion_01 +Cognicion_02+Escala_01+Escala_02"),
    data = . , 
    strata = Var_respuesta )  %>% 
  # üî¥ 2. step
  # üü° En este caso no aplicamos ninguna transformaci√≥n 
  # Step_log
  # üî¥ 3. Prep (asienta la receta y general√≠zala) 
  prep() %>% 
  # üî¥ 4. Bake (pon la receta en producci√≥n/comprueba qu√© tal ha funcionado)
  bake(., new_data=NULL) %>% 
  # displayment
  head(.,10) %>% 
  knitr::kable()
```



## 6.2.2. Imputaci√≥n m√∫ltiple {auto-animate="true"}

```{r}
Training_datos %>% 
  # üî¥ 1. Recipe (crea la f√≥rmula generalizada del modelo) 
  recipe(  
    as.formula("Var_respuesta ~ Cognicion_01 +Cognicion_02+Escala_01+Escala_02"),
    data = . , 
    strata = Var_respuesta )  %>% 
  # üî¥ 2. Step (haz la transformaci√≥n que requieras) 
  # üü° Este step ser√° la imputaci√≥n por regresi√≥pn lineal con otras covariables
  step_impute_linear( Cognicion_02, impute_with = imp_vars(Cognicion_01, Escala_02) ) %>%
  # üî¥ 3. Prep (asienta la receta y general√≠zala) 
  prep() %>%
  # üî¥ 4. Bake (pon la receta en producci√≥n/comprueba qu√© tal ha funcionado)
  bake(., new_data=NULL) %>%   
  # displayment
  head(.,10) %>% 
  knitr::kable()
```


## 6.2.3. PCA {auto-animate="true"}

```{r}
Training_datos %>% 
  # üî¥ 1. Recipe (crea la f√≥rmula generalizada del modelo) 
  recipe(  
    as.formula("Var_respuesta ~ Cognicion_01 + Cognicion_02 + Cognicion_03 + Escala_01 + Escala_02  + Escala_03"),
    data = . , 
    strata = Var_respuesta )  %>% 
  # üî¥ 2. Step (haz la transformaci√≥n que requieras) 
  # üü° Este step ser√° la imputaci√≥n por regresi√≥n lineal con otras covariables
  step_impute_bag(all_numeric()) %>% 
  step_pca( Cognicion_01 , Cognicion_02 , Cognicion_03, num_comp = 2,  id = "pca") %>%
  # üî¥ 3. Prep (asienta la receta y general√≠zala) 
  prep() %>%
  # üî¥ 4. Bake (pon la receta en producci√≥n/comprueba qu√© tal ha funcionado)
  tidy(id = "pca") %>%
  # displayment
  head(.,10) %>% 
  knitr::kable()
```


## 6.2.4. Remuestrear {auto-animate="true"}

```{r}
# üî¥ Librer√≠a que permite instalar algoritmos y hacer sobremuestreo en Tidymodels.
pacman::p_load(themis)

Training_datos_adasyn <- Training_datos %>% 
  # 1. Recipe (crea la f√≥rmula generalizada del modelo) 
  recipe(  
    as.formula("Var_respuesta ~ Cognicion_01 +Cognicion_02+Escala_01+Escala_02"),
    data = . , 
    strata = Var_respuesta )  %>% 
  #  2. Step (haz la transoformaci√≥n que requieras) 
  #  step de  imputaci√≥n por √°rboles manteniendo la estructura de todas las covariables
  step_impute_bag(all_numeric()) %>% 
  # üî¥ step para sobremuestrear el nivel menor en la variable respuesta.
  step_adasyn(Var_respuesta, over_ratio = 1, neighbors = 10) %>% 
  #  3. Prep (asienta la receta y general√≠zala) 
  prep() %>%
  #  4. Bake (pon la receta en producci√≥n/comprueba qu√© tal ha funcionado)
  bake(., new_data=NULL)
```


## 6.2.4. Remuestrear {auto-animate="true"}

```{r}
ggpubr::ggarrange(
  Training_datos %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      ungroup() %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(
        aes(label = scales::percent(prop_var_respuesta)), 
        position = position_stack(vjust = 0.5), size = 4) +
    scale_fill_manual( values=c("azure4", "azure3"))
    ,
    Training_datos_adasyn %>% 
      group_by(Var_respuesta) %>% 
      summarise(recuento = n()) %>% 
      mutate(prop_var_respuesta = recuento / sum(recuento) ) %>% 
      ungroup() %>% 
      ggplot(aes(x = Var_respuesta, y = recuento, fill = Var_respuesta)) +
      geom_bar(stat = "identity") +
      geom_text(
        aes(label = scales::percent(prop_var_respuesta)),
        position = position_stack(vjust = 0.5), size = 4) +
      scale_fill_manual( values=c("azure4", "azure3")),
    common.legend = T, legend = 'bottom'
  ,
  labels = c('Normal', 'Sobre Muestreo\n        Adasyn'), label.x = 0.1
) %>%  
  plot()
```

## 6.3. Caso pr√°ctico: Receta {auto-animate="true"}

Por el momento dejar√© una receta lista para continuar con el taller.

```{r caso-practico-receta, echo=TRUE}
# Creamos la F√≥rmula para la receta a partir 
# de nombres de las variables y un poco de magia con paste()
Receta_modelo_1_formula <- Training_datos %>% 
  dplyr::select(
   dplyr::matches('Cog'), dplyr::matches('escala')) %>% 
  names() %>% 
  paste(., collapse = ' + ') %>% 
  paste('Var_respuesta ~' ,.) %>% 
  as.formula() 

  # üî¥ 1.) Iniciamos la receta
Receta_modelo_1 <- recipe(  
    formula = Receta_modelo_1_formula,
    data = Training_datos , 
    strata = Var_respuesta)  %>% 
  # üî¥ 2.) Steps
    #  üü° 2.1) Eliminamos las variables que contengan m√°s de un 20% de datos perdidos 
  step_filter_missing(all_predictors(),  threshold = 0.1) %>% 
    #  üü° 2.2) Imputamos las variables num√©ricas con un algoritmo de bagged trees
  step_impute_bag(all_numeric(),-all_outcomes()) %>%
    # 2.3) Normalizamos los datos (restamos la media)
  step_normalize(all_numeric(),-all_outcomes() ) %>% 
    # 2.4) Escalamos los datos (reducimos a escala entre 0 y 1)
  step_scale(all_numeric(),-all_outcomes()) %>% 
    # 2.5) Convertimos a Dummy las variables factor (no har√° nada por que no tenemos variables factor)
  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>% 
    # 2.6) Sobre muestreamos la variable para equilibrar grupos
  step_adasyn(Var_respuesta, over_ratio = 1, neighbors = 10) %>% 
  # 3.) Preparamos la receta
  prep()
```

::: titulo-seccion
## 7. ¬øD√≥nde est√° el modelo? Creando modelos con Parsnip
:::

## 7.1. Modelos a la carrera con Parsnip {auto-animate="true"}

![](Imagenes/parsnip_img.jpeg){fig-align="center"}

## 7.1. Modelos a la carrera con Parsnip {auto-animate="true"}

![](Imagenes/search_parsnip_models.png){fig-align="center"}
https://www.tidymodels.org/find/parsnip/

## 7.2. Empezamos a modelar {auto-animate="true"}

```{r echo=TRUE, eval=FALSE}
# üî¥ Ver los posibles motores de un tipo de modelo
show_engines('logistic_reg') 


logistic_reg(
  engine = "glmnet",
  #Hiperpar√°metros           
  penalty = NULL, mixture = NULL, mode = 'classification')


logistic_reg(
  engine = "glmnet",
  #Hiperpar√°metros           
  penalty = 0, mixture = 0, mode = 'classification')


logistic_reg(
  engine = "glmnet",
  #Hiperpar√°metros           
  penalty = tune(), mixture = tune(), mode = 'classification')
```

## 7.3. Caso pr√°ctico: modelo de Random Forest {auto-animate="true"}

```{r echo=TRUE}
Model_RandomForest <- 
  # üî¥ Especificamos el modelo que queremos, en este caso un random forest
  rand_forest(
  # üü° Ajustamos los hiperpar√°metros 
    mtry = tune(),  trees = tune(),  min_n = tune()) %>% 
  # üî¥ Ponemos el motor, es decir, la librer√≠a por la queremos que se ejecute el modelo
  set_engine("ranger", importance = "impurity") %>% 
  # üî¥ Ajustamos el modo del modelo, es decir, si queremos regresi√≥n o clasifiaci√≥n 
  set_mode("classification")
```


::: titulo-seccion
## 8. Unificando todo: Workflow
:::

## 8. Unificando todo: Workflow

```{r echo=TRUE}
WF_Receta_modelo_1_Random_forest <- 
  # üî¥ Activamos el workflow 
  workflow() %>% 
  # üî¥ A√±adimos la receta
  add_recipe(Receta_modelo_1) %>% 
  # üî¥ A√±adimos el modelo
  add_model(Model_RandomForest)
```

::: titulo-seccion
## 9. Eligiendo m√©tricas con Yardstick {auto-animate="true"}
:::

## 9. Eligiendo m√©tricas con Yardstick {auto-animate="true"}
```{r echo=TRUE, eval=FALSE}
# üî¥ Seleccionamos las m√©tricas que queremos para nuestro modelo
Modelo_Metricas <- metric_set(accuracy, j_index, precision, sensitivity, specificity, roc_auc, f_meas,recall, mcc)
```

Se puede encontrar m√°s informaci√≥n en: https://yardstick.tidymodels.org/articles/custom-metrics.html

::: titulo-seccion
## 10. Eligiendo hiperpar√°metros: Tune y Dials
:::

## 10.1. Eligiendo hiperpar√°metros: Tune y Dials {auto-animate="true"}

:::incremental
- Los hiperpar√°metros no pueden aprenderse a partir de los datos, pero que influyen significativamente en el rendimiento del modelo. 

- A diferencia de los par√°metros del modelo, que se aprenden durante el entrenamiento, los hiperpar√°metros se establecen antes de que comience el entrenamiento.

- La selecci√≥n de los hiperpar√°metros adecuados es esencial para conseguir un modelo generalizable y con buen rendimiento.

- Los hiperpar√°metros desempe√±an un papel crucial a la hora de controlar el equilibrio entre la sobreadaptaci√≥n y la inadaptaci√≥n (oiverfitting y underfitting). 

- Ajustar los hiperpar√°metros puede ayudar a encontrar el nivel adecuado de complejidad del modelo, haciendo un buen equilibrio entre la varianza del conjunto de entrenamiento y el de evaluaci√≥n.
:::

## 10.1. Eligiendo hiperpar√°metros: Tune y Dials {auto-animate="true"}

- Los paquetes Tune y Dials son los que nos permiten hacer esta selecci√≥n y control. Aqu√≠ se crea todo un subflujo de trabajo para el ajuste de hiperpar√°metros.

::: columns
::: {.column width="30%"}
![](Imagenes/logo_tune.png){fig-align="center"}

![](Imagenes/logo_dials.png){fig-align="center"}
:::

::: {.column width="30%"}
1. Creamos unas r√©plicas del conjunto de entrenamiento, para tener subconjunto de entrenamiento y de validaci√≥n (que ya hicimos en el apartado de Rsample con el objeto ‚Äú‚Äú)
2. Elegimos un modelo con Parsnip y dejamos sus hiperpar√°metros en abierto con `tune()`.
3. Creamos una cuadr√≠cula (grid) con diferentes posibles combinaciones de los hiperpar√°metros que dejamos en abierto en el paso 2.
4. Ponemos todo junto con la funci√≥n `tune_grid()`.
:::
::: 


## 10.2. Montando un espacio de hiperpar√°metros: familia `grid_` {auto-animate="true"}

:::incremental
- `grid_regular`: har√° todas las combinaciones entre los rangos de la variable.
- `grid_random`: probar√° hiperpar√°metros aleatoriamente.
- `grid_max_entropy`: propondr√° una combinaci√≥n de hiperp√°rametros que garanticen que se cubrir√° todo el espectro, con fin de evitar m√≠nimos locales en la conversi√≥n de algoritmos.
- `grid_latin_hypercube`: propondr√° una matriz.
:::

## 10.2.1. Grid_regular {auto-animate="true"}

- `Grid_regular` har√° las combinaciones acorde al rango que le hayamos dado. Por defecto hace la combinaci√≥n entre el m√≠nimo, el medio y el m√°ximo de cada valor.

```{r rid-regular-1, echo=TRUE}

grid_regular(
  mtry(range  = c(2, 10)),
  min_n(range = c(2, 10)),
  trees(range = c(2, 10)))

```

## 10.2.1. Grid_regular() {auto-animate="true"}

- `Grid_regular` har√° las combinaciones acorde al rango que le hayamos dado. Por defecto hace la combinaci√≥n entre el m√≠nimo, el medio y el m√°ximo de cada valor.

```{r rid-regular-2, echo=TRUE}

grid_regular(
  mtry(range  = c(2, 10)),
  min_n(range = c(2, 10)),
  trees(range = c(2, 10)),
  levels = 4) # Podemos ampliar con el par√°metro levels: Ahora el m√≠nimo ser√° el cuantil 33 y el m√°ximo el cuantil 66.

```

## 10.2.2. Grid_max_entropy() {auto-animate="true"}

```{r rid-max-entr, echo=TRUE}

grid_max_entropy(
  mtry(range  = c(1, 4)),
  min_n(range = c(10, 30)),
  trees(range = c(1, 1000)),
  size = 10
  )
```

## 10.3. Ajustando los hiperpar√°metros: convergen Yardstick con dial y tune {auto-animate="true"}

```{r hiperpar-1, echo=TRUE}

WF_hiper_parametros <- 
  # üî¥ Este proceso se hace con la funci√≥n tune_grid()üî¥
  tune_grid(
  # Ponemos la receta de nuestro modelo, que incuye el preprocesamiento y la f√≥rmula
  object = WF_Receta_modelo_1_Random_forest,
  # üü° Ponemos los Folds del conjunto de entrenamiento,  
  # üü° Dentro del entrenamiento habr√° un sub-entrenamiento y una sub-validaci√≥n
  resamples = Folds_training,
  #Ponemos un grid para que pruebe con diferentes hiperpar√°metros, ya con un rango predefinido
  grid = grid_max_entropy(
    mtry(range  = c(1, 4)),
    min_n(range = c(10, 30)),
    trees(range = c(1, 1000)),
    size = 10),
  # Ponemos las m√©tricas que hemos definido anteriormente
  metrics = Modelo_Metricas, 
  # Con esta opci√≥n podemos guardar las predicciones
  control = control_grid( save_pred = T)
)
```

## 10.3. Ajustando los hiperpar√°metros: convergen Yardstick con dial y tune {auto-animate="true"}

```{r hiperpar-2, echo=TRUE}
WF_hiperparametros_coleccion <- 
  WF_hiper_parametros %>% 
  collect_metrics() # üî¥ Esta es la funci√≥n que retorna las m√©tricas, importante tenerla en cuenta

WF_hiperparametros_coleccion %>% 
  head(.,20) %>% 
  knitr::kable()
```

## 10.3. Ajustando los hiperpar√°metros: convergen Yardstick con dial y tune {auto-animate="true"}

```{r hiperpar-3, echo=TRUE}
autoplot(WF_hiper_parametros)
```

## 10.3. Ajustando los hiperpar√°metros: convergen Yardstick con dial y tune {auto-animate="true"}

```{r hiperpar-4, echo=TRUE}
WF_hiper_parametros %>% 
  collect_metrics() %>% 
  pivot_longer(cols = c('mtry','trees', 'min_n'), names_to = 'tipo_parametro', values_to = 'valor_parametro') %>% 
  ggplot(aes(valor_parametro,mean, color=mean)) +
  geom_point() +
  facet_grid(.metric~tipo_parametro, scales = 'free' ) +
  scale_color_viridis_b()
```


## 10.3. Ajustando los hiperpar√°metros: convergen Yardstick con dial y tune {auto-animate="true"}

```{r hiperpar-5, echo=TRUE}
WF_hiperparametros_mejor <- WF_hiper_parametros %>%  show_best("roc_auc",1)

WF_hiperparametros_mejor %>% 
  knitr::kable()
```


::: titulo-seccion
## 11. La conversi√≥n de todo: `fit()` y `predict()`
:::

## 11. La conversi√≥n de todo: `fit()` y `predict()` {auto-animate="true"}

```{r modelo-fitted, echo=TRUE}
Modelo_fitted <- WF_Receta_modelo_1_Random_forest %>% 
  # üî¥ Finalizamos el flujo de trabajo con la mejor combinaci√≥n de Hiperpar√°metros encontrada
  finalize_workflow(WF_hiperparametros_mejor) %>%
  # üî¥ Finalmente ajustamos con fit()
  fit(data= Training_datos)
```

## 11. La conversi√≥n de todo: `fit()` y `predict()` {auto-animate="true"}

```{r nuevo-caso-modelo, echo=TRUE}
Nuevo_caso <- tibble(
  'Cognicion_01' = 123,
  'Cognicion_02' = 102,
  'Cognicion_03' = 89,
  'Cognicion_04' = 100,
  'Cognicion_05' = 179,
  'Escala_01'    = 12,
  'Escala_02'    = 22,
  'Escala_03'    = 20
)
```

## 11. La conversi√≥n de todo: `fit()` y `predict()` {auto-animate="true"}

```{r predict-modelo, echo=TRUE}
tibble(
  # üî¥ La funci√≥n predict() de la librer√≠a stats tiene sinergias con Tidymodels, necesita de estos argumentos:
  # - üü° El modelo ya ajustado (model_fitted) 
  # - üü° Un tibble/dataframe con la nueva predicci√≥n (debe contener las mismas variables)
  # - üü° Un tipo de predicci√≥n en los casos de clasificaci√≥n, si queremos las probabilidades de cada categor√≠a o la categor√≠a predicha  directamente
  predict(Modelo_fitted, Nuevo_caso, type = "prob" ),
  predict(Modelo_fitted, Nuevo_caso, type = "class")) %>% 
  knitr::kable()v

```

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

:::incremental
- La funci√≥n `last_fit()` permite tener todo unificado en un solo tibble. Recibe directamente los datos de entrenamiento y los datos de evaulaci√≥n y crea sus m√©tricas y sus predicciones.

- Contamos con una serie de funciones para extraer todo lo que necesitemos de ellas: 
1. `extract_fit_engine()`: permite extraer el modelo crudo como si no hubiera sido ajustado con tidymodels.
2. `collect_metrics()`: extraer un tibble con las m√©tricas del modelo (puestas con metric set) 
3. `collect_predictions()`: extrae un tibble con las predicciones de los datos de evaluaci√≥n. 
4. `extract_workflow()`: Permite extraer todo el flujo del modelo, incluyendo la receta de modo que podemos hacer predicciones:
:::

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

4. `extract_workflow()`: Permite extraer todo el flujo del modelo, incluyendo la receta de modo que podemos hacer predicciones:

```{r extract-workflow-1, echo=TRUE}
Modelo_1_final <-  WF_Receta_modelo_1_Random_forest %>% 
  # üî¥ Finalizamos el flujo de trabajo con la mejor combinaci√≥n de hiperpar√°metros encontrada
  finalize_workflow(WF_hiperparametros_mejor) %>%
  # üî¥ Ajustamos con last_fit(), poniendo el split de datos inicial y las m√©tricas para las predicciones
  last_fit(Split_datos_strat, metrics = Modelo_Metricas)

Modelo_1_final
```

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

4. `extract_workflow()`: Permite extraer todo el flujo del modelo, incluyendo la receta de modo que podemos hacer predicciones:

```{r extract-workflow-2, echo=TRUE}
Modelo_1_final <-  WF_Receta_modelo_1_Random_forest %>% 
  # üî¥ Finalizamos el flujo de trabajo con la mejor combinaci√≥n de hiperpar√°metros encontrada
  finalize_workflow(WF_hiperparametros_mejor) %>%
  # üî¥ Ajustamos con last_fit(), poniendo el split de datos inicial y las m√©tricas para las predicciones
  last_fit(Split_datos_strat, metrics = Modelo_Metricas) %>% 
  # üî¥ dentro del flujo de trabajo est√°n la receta y el modelo ya ajustado
  extract_workflow() %>% 
  # de modo que lo podemos usar para hacer predicciones
  predict(Testing_datos, type = "prob" )

Modelo_1_final
```

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

- A partir de aqu√≠ podemos manipular el objeto fit de m√∫ltiples maneras m√∫ltiples:

```{r extract-workflow-3, echo=TRUE}
Modelo_1_final %>%
  # üî¥ Con esta funci√≥n podemos recopilar todas las m√©tricas de un modelo
  collect_metrics() %>% 
  select(-.estimator,-.config) %>%  
  mutate(.estimate= round(.estimate,2)) %>% 
  knitr::kable()
```

## 11.1. La funci√≥n `last_fit()` {auto-animate="true"}

```{r curva-roc, echo=FALSE}
options(yardstick.event_first = FALSE)

Modelo_1_final %>%  collect_predictions() %>% 
  # üî¥ Esta funcion permite calcular rapidamente la curva roc de un modelo ya ajustado
  roc_curve(truth = Var_respuesta,  .pred_0 ) %>% 
  ggplot(aes( x = 1- specificity ,y= sensitivity ) ) +
  geom_path() +
  geom_abline(intercept=0, slope=1, linetype=3) +
  labs(title= 'Cruva roc del modelo_1_final')
```

Advertencia: en la funci√≥n de curva roc, ved que he puesto predicci√≥n del 0 en vez del 1. Esto es un error interno en la clasificaci√≥n dentro del paquete yardstick. Hay que tener cuidado sobre si estas funciones utilizan el primer / segundo / x nivel de su factor como el ‚Äúevento‚Äù. Por defecto, yardstick elige el primer nivel de verdad como ‚Äúevento‚Äù al calcular la curva roc. Pod√©is encontrar m√°s de este suceso en el github de los desarrolladores https://github.com/tidymodels/yardstick/issues/94


::: titulo-seccion
## 12. Rendimiento del umbral (Threshold performance en modelos de clasificaci√≥n)
:::

## 12. Rendimiento del umbral (Threshold performance en modelos de clasificaci√≥n) {auto-animate="true"}

:::incremental
- El concepto de rendimiento del umbral (threshold performance) es crucial a la hora de evaluar y ajustar modelos de clasificaci√≥n binaria.

- El umbral es el valor de probabilidad por encima del cual un resultado predicho se considera de clase positiva. Por defecto, muchos modelos utilizan un umbral de 0,5.

- Ajustar el umbral permite elegir entre precisi√≥n y la recall. Bajar el umbral aumenta la sensibilidad (recall) pero disminuye la precisi√≥n, y viceversa.

- En tidymodels est√° la librer√≠a `probably` con la funci√≥n `threshold_perf()`, la cual nos permite, dadas las predicciones del modelo, ver como se desenvuelve este umbral.
:::

## 12. Rendimiento del umbral (Threshold performance en modelos de clasificaci√≥n) {auto-animate="true"}

```{r Threshold-1, echo=TRUE}
pacman::p_load(probably)


Modelo_1_final_Threshold_performance <- 
  # Llamamos al modelo ya finalizado con "last_fit()"
  Modelo_1_final %>% 
  # Coleccionamos sus predicciones
  collect_predictions() %>% 
  # üî¥ Ejecutamos el rendimeinto de umbral con la siguiente funci√≥n
  threshold_perf(
    # Le decimos qu√© variable es la verdadera respuesta
    truth = Var_respuesta,  
    # Le decimos qu√© vairbale es la predicci√≥n (aviso: debido a un bug en la version )
    .pred_1, 
    # ajustamos un rango en el que se evaluar√° el umbral en cada punto
    thresholds = seq(0.5,1,0.01) )

# Displayment
Modelo_1_final_Threshold_performance %>% 
  head(.,10) %>% 
  mutate(.estimate= round(.estimate,2)) %>% 
  knitr::kable()
```

## 12. Rendimiento del umbral (Threshold performance en modelos de clasificaci√≥n) {auto-animate="true"}

```{r Threshold-2, echo=FALSE}
Modelo_1_final_Threshold_performance %>% 
  ggplot(aes(x=.threshold,y=.estimate ,color=.metric)) +
  geom_line(size=1.1)+
  scale_color_manual(values=c('purple','blue','pink'))+
  geom_vline(xintercept = 
    Modelo_1_final_Threshold_performance %>%  
      filter(.metric=='j_index') %>%
      arrange(desc(.estimate)) %>% 
      slice(1) %>% 
      pull(.threshold), linetype = 'dashed', size=1.5)
```

::: titulo-seccion
## 13. El cielo es el l√≠mite: `workflow_set()`
:::

## 13.1. Todas las recetas {auto-animate="true"}

```{r todas-recetas, echo=TRUE}
Receta_modelo_Escalas_formula <- Training_datos %>% 
  dplyr::select(dplyr::matches('Esc')) %>% 
  names() %>% 
  paste(., collapse = ' + ') %>% 
  paste('Var_respuesta ~' ,.) %>% 
  as.formula() 

  # üî¥ 1.) Iniciamos la receta
Receta_modelo_Escalas <- recipe(  
    formula = Receta_modelo_Escalas_formula,
    data = Training_datos , 
    strata = Var_respuesta)  %>% 
  # üî¥ 2.) Steps
    #  üü° 2.1) Eliminamos las variables que contengan m√°s de un 25% de datos perdidos 
  step_filter_missing(all_predictors(),  threshold = 0.25) %>% 
    #  üü° 2.2) Imputamos las variables num√©ricas con un algoritmo de bagged trees
  step_impute_bag(all_numeric(),-all_outcomes()) %>%
    # 2.3) Normalizamos los datos (restamos la media)
  step_normalize(all_numeric(),-all_outcomes() ) %>% 
    # 2.4) Escalamos los datos (reducimos a escala entre 0 y 1)
  step_scale(all_numeric(),-all_outcomes()) %>% 
    # 2.5) Convertimos a Dummy las variables factor (no har√° nada por que no tenemos variables factor)
  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>% 
    # 2.6) Sobremuestreamos la variable para equilibrar grupos
  step_adasyn(Var_respuesta, over_ratio = 1, neighbors = 10) %>% 
  # 3.) Preparamos la receta
  prep()



Receta_modelo_Cognicion_formula <- Training_datos %>% 
  dplyr::select(
   dplyr::matches('Cog')) %>% 
  names() %>% 
  paste(., collapse = ' + ') %>% 
  paste('Var_respuesta ~' ,.) %>% 
  as.formula() 

  # üî¥ 1.) Iniciamos la receta
Receta_modelo_Cognicion <- recipe(  
    formula = Receta_modelo_Cognicion_formula,
    data = Training_datos , 
    strata = Var_respuesta)  %>% 
  # üî¥ 2.) Steps
    #  üü° 2.1) Eliminamos las variables que contengan m√°s de un 25% de datos perdidos 
  step_filter_missing(all_predictors(),  threshold = 0.25) %>%
    #  üü° 2.2) Imputamos las variables num√©ricas con un algoritmo de bagged trees
  step_impute_mean(all_numeric(),-all_outcomes()) %>%
    # 2.3) Normalizamos los datos (restamos la media)
  step_normalize(all_numeric(),-all_outcomes() ) %>% 
    # 2.4) Escalamos los datos (reducimos a escala entre 0 y 1)
  step_scale(all_numeric(),-all_outcomes()) %>% 
    # 2.5) Convertimos a Dummy las variables factor (no har√° nada por que no tenemos variables factor)
  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>% 
    # 2.6) Sobremuestreamos la variable para equilibrar grupos
  step_adasyn(Var_respuesta, over_ratio = 1, neighbors = 10) %>% 
  # 3.) Preparamos la receta
  prep()

bake(Receta_modelo_Cognicion, new_data = NULL)
```


## 13.2. `workflow_set()` para unificarlas todas {auto-animate="true"}

```{r workflow-set-1, echo=TRUE}
Receta_list <- list(
  'Modelo_Escalas'    = Receta_modelo_Escalas,
  'Modelo_Cognicion'  = Receta_modelo_Cognicion,
  'Modelo_Bioquimica' = Receta_modelo_Bioquimica,
  'Modelo_Popurri'    = Receta_modelo_Popurri
  )
  
Model_list <- list(
  RandomForest     = Model_RandomForest
  # ,XGBoost          = Model_XGBoost
  # ,Bagged_Trees     = Model_Bagged_Tree
  )

Wokflows_set <- workflow_set(
  preproc = Receta_list, 
  models = Model_list, 
  # üî¥ La opci√≥n Cross es para que haga todos los cruces de modelos con recetas
  cross = T)


Wokflows_set
```

## 13.2. `workflow_set()` para unificarlas todas {auto-animate="true"}

```{r workflow-set-2, echo=TRUE}
Wokflows_set_map <- 
  Wokflows_set %>% 
  # üî¥ workflow_map() es una funci√≥n que permite ajustar m√∫ltiples flujos de trabajo
    workflow_map(
    resamples = Folds_training, 
    fn = "tune_grid",
    grid = grid_max_entropy(
      mtry(range  = c(1, 4)),
      min_n(range = c(10, 30)),
      trees(range = c(1, 1000)),
      size = 10),
    # verbose = TRUE, 
    metrics = Modelo_Metricas, 
    control = control_grid( save_pred = T),
    # üî¥ para garantizar replicabildiad, podemos fijar la semilla en el proceso
    seed = 2465)

Wokflows_set_map
```

## 13.2. `workflow_set()` para unificarlas todas {auto-animate="true"}

```{r workflow-set-3, echo=TRUE, eval=FALSE}

# Modelo Escalas ----
Modelo_Escalas_RandomForest_hyperparametros <- Wokflows_set_map %>% 
  # üî¥ Extraemos el flujo correspondiente al modelo que queremos 
  extract_workflow_set_result('Modelo_Escalas_RandomForest') %>%
  # üî¥ podemos ver una tabla con los mejores resultados, acorde a una m√©trica
  show_best(metric = 'sensitivity', n=50) %>% 
  filter(row_number()==1) %>% 
  select(mtry,trees,min_n,.config )

Modelo_Escalas_RandomForest_final <- Wokflows_set_map %>% 
  # üî¥ Extraemos el flujo correspondiente al modelo que queremos 
  extract_workflow('Modelo_Escalas_RandomForest') %>% 
  finalize_workflow(Modelo_Escalas_RandomForest_hyperparametros) %>%
  last_fit(Split_datos_strat, metrics = Modelo_Metricas )

# Modelo Cognici√≥n ----

Modelo_Cognicion_RandomForest_hyperparametros <- Wokflows_set_map %>% 
  extract_workflow_set_result('Modelo_Cognicion_RandomForest') %>%
  show_best(metric = 'sensitivity', n=50) %>% 
  filter(row_number()==1) %>% 
  select(mtry,trees,min_n,.config )

Modelo_Cognicion_RandomForest_final <- Wokflows_set_map %>% 
  extract_workflow('Modelo_Cognicion_RandomForest') %>% 
  finalize_workflow(Modelo_Cognicion_RandomForest_hyperparametros) %>%
  last_fit(Split_datos_strat, metrics = Modelo_Metricas )

# Modelo Bioqu√≠mico ----

Modelo_Bioquimica_RandomForest_hyperparametros <- Wokflows_set_map %>% 
  extract_workflow_set_result('Modelo_Bioquimica_RandomForest') %>%
  show_best(metric = 'sensitivity', n=50) %>% 
  filter(row_number()==1) %>% 
  select(mtry,trees,min_n,.config )

Modelo_Bioquimica_RandomForest_final <- Wokflows_set_map %>% 
  extract_workflow('Modelo_Bioquimica_RandomForest') %>% 
  finalize_workflow(Modelo_Bioquimica_RandomForest_hyperparametros) %>%
  last_fit(Split_datos_strat, metrics = Modelo_Metricas )

# Modelo Popurri ----
Modelo_Popurri_RandomForest_hyperparametros <- Wokflows_set_map %>% 
  extract_workflow_set_result('Modelo_Popurri_RandomForest') %>%
  show_best(metric = 'sensitivity', n=50) %>% 
  filter(row_number()==1) %>% 
  select(mtry,trees,min_n,.config )

Modelo_Popurri_RandomForest_final <- Wokflows_set_map %>% 
  extract_workflow('Modelo_Popurri_RandomForest') %>% 
  finalize_workflow(Modelo_Popurri_RandomForest_hyperparametros) %>%
  last_fit(Split_datos_strat, metrics = Modelo_Metricas )
```


## 13.3. M√©tricas de todos los modelos {auto-animate="true"}

```{r metricas-1, echo=TRUE}
map2(
  list(
    Modelo_Escalas_RandomForest_final,
    Modelo_Cognicion_RandomForest_final,
    Modelo_Bioquimica_RandomForest_final,
    Modelo_Popurri_RandomForest_final),
  list('Modelo_Escala','Modelo_Cognicion','Modelo_Bioquimica','Modelo_Popurri'),
  ~ ..1 %>% 
    collect_metrics() %>% 
    mutate('Modelo'= ..2) %>% 
    select(.metric,.estimate,Modelo )) %>% 
  bind_rows() %>% 
  pivot_wider(names_from = Modelo, values_from = .estimate) %>% 
  knitr::kable()
```

## 13.4. Curvas roc de todos los modelos {auto-animate="true"}

```{r curvas-roc-todos-modelos, echo=FALSE}
ggpubr::ggarrange( Plot_ROC_CURVE_training_todos_los_posibles_modelos,
                   Plot_ROC_CURVE_training_modelos_finales, 
                   Plot_ROC_CURVE_testing_modelos_finales ,
                   nrow=1 , 
                   common.legend = T,
                   legend="bottom",
                   labels = c('Todos_training','Training','Test'))
```


::: titulo-seccion
## 14. Haciendo el machine learning entendible: SHAP values
:::

## 14. Haciendo el machine learning entendible: SHAP values

:::incremental
- Los valores de Shapley (shap values), que deben su nombre al premio Nobel Lloyd Shapley, son un concepto de la teor√≠a de juegos cooperativos que ha acabado encontrado aplicaciones en el aprendizaje autom√°tico.

- En teor√≠a de juegos, los valores de Shapley son la distribuci√≥n equitativa de la contribuci√≥n de cada jugador en un juego cooperativo entre todos los jugadores. En el contexto del aprendizaje autom√°tico, las caracter√≠sticas o variables se consideran jugadores, y el juego consiste en cu√°nto contribuye cada caracter√≠stica a la predicci√≥n de un modelo.

- Es decir, los valores de Shapley son sencillamente las marginales de cada variable dentro del modelo. 

- El valor final llamado Shap value es promedio de todas las aportaciones de una variable marginal.

- Aparte de su extenso coste computacional, todo shap value asume que est√° utilizando un modelo de caja negra, de manera que es posible que el modelo est√© dando indicaciones err√≥neas.

- Actualmente se pueden compilar en tidymodels utilizando la libreria agua, al ser una extensi√≥n de Tidymodels del entorno de h2o. 

- Una propuesta ser√≠a buscar un modelo bien calibrado con Tidymodels y pasarlo por h2o para hacer un diagn√≥stico m√°s eficaz.
:::

